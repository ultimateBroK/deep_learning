{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ M√¥ H√¨nh D·ª± B√°o Gi√° Bitcoin V·ªõi BiLSTM\n",
    "\n",
    "Notebook n√†y h∆∞·ªõng d·∫´n b·∫°n t·ª´ng b∆∞·ªõc ƒë·ªÉ x√¢y d·ª±ng v√† hu·∫•n luy·ªán m√¥ h√¨nh d·ª± b√°o gi√° Bitcoin.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Checklist\n",
    "\n",
    "- [ ] B∆∞·ªõc 1: L·∫•y d·ªØ li·ªáu t·ª´ Binance\n",
    "- [ ] B∆∞·ªõc 2: X·ª≠ l√Ω d·ªØ li·ªáu\n",
    "- [ ] B∆∞·ªõc 3: X√¢y d·ª±ng model BiLSTM\n",
    "- [ ] B∆∞·ªõc 4: Training model\n",
    "- [ ] B∆∞·ªõc 5: ƒê√°nh gi√° & V·∫Ω bi·ªÉu ƒë·ªì\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Tips\n",
    "\n",
    "1. Ch·∫°y t·ª´ng cell theo th·ª© t·ª± t·ª´ tr√™n xu·ªëng d∆∞·ªõi\n",
    "2. ƒê·ªçc comments trong code ƒë·ªÉ hi·ªÉu\n",
    "3. N·∫øu c·∫ßn gi·∫£i th√≠ch chi ti·∫øt, xem `docs/` folder\n",
    "4. Ngh·ªâ gi·∫£i lao n·∫øu c·∫£m th·∫•y ng·ª£p!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 0. Setup & C·∫•u H√¨nh\n",
    "\n",
    "C·∫•u h√¨nh TensorFlow v√† import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress TensorFlow warnings BEFORE importing TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Ch·ªâ hi·ªÉn th·ªã ERROR\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # T·∫Øt oneDNN warnings\n",
    "\n",
    "# Suppress Python warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', message='.*np.object.*')\n",
    "warnings.filterwarnings('ignore', message='.*oneDNN.*')\n",
    "warnings.filterwarnings('ignore', message='.*CUDA.*')\n",
    "warnings.filterwarnings('ignore', message='.*Could not find cuda.*')\n",
    "warnings.filterwarnings('ignore', message='.*cuda drivers.*')\n",
    "warnings.filterwarnings('ignore', message='.*GPU will not be used.*')\n",
    "\n",
    "# Third-party imports (must be after env vars for TensorFlow)\n",
    "import numpy as np  # noqa: E402\n",
    "import polars as pl  # noqa: E402\n",
    "import tensorflow as tf  # noqa: E402\n",
    "\n",
    "# Suppress TensorFlow logger after import\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# Setup project path (notebook runs from notebooks/, need to go up one level)\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Local imports\n",
    "from src.runtime import (  # noqa: E402\n",
    "    configure_tensorflow_runtime,\n",
    "    print_tensorflow_info,\n",
    "    set_random_seed\n",
    ")\n",
    "\n",
    "# C·ªë ƒë·ªãnh seed ƒë·ªÉ t√°i l·∫≠p k·∫øt qu·∫£ (n·∫øu SEED < 0 th√¨ b·ªè qua)\n",
    "try:\n",
    "    set_random_seed(SEED, deterministic=True)\n",
    "except NameError:\n",
    "    # N·∫øu cell c·∫•u h√¨nh ch∆∞a ch·∫°y, fallback seed m·∫∑c ƒë·ªãnh\n",
    "    set_random_seed(42, deterministic=True)\n",
    "\n",
    "# C·∫•u h√¨nh TensorFlow cho CPU AMD\n",
    "configure_tensorflow_runtime(\n",
    "    intra_op_threads=12,\n",
    "    inter_op_threads=2,\n",
    "    enable_xla=True\n",
    ")\n",
    "\n",
    "# In th√¥ng tin TensorFlow\n",
    "print_tensorflow_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 1. C·∫•u H√¨nh Tham S·ªë\n",
    "\n",
    "Thay ƒë·ªïi c√°c tham s·ªë t√πy √Ω:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== C·∫§U H√åNH ====================\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42  # ƒê·ªïi seed ƒë·ªÉ ch·∫°y th·ª≠ nghi·ªám; ƒë·∫∑t <0 ƒë·ªÉ kh√¥ng c·ªë ƒë·ªãnh\n",
    "\n",
    "# Symbol\n",
    "SYMBOL = \"BTC/USDT\"  # Trading pair symbol\n",
    "\n",
    "# Data parameters (CSV local)\n",
    "# S·ª≠ d·ª•ng project_root ƒë·ªÉ t√¨m ƒë∆∞·ªùng d·∫´n ƒë√∫ng (notebook ch·∫°y t·ª´ notebooks/)\n",
    "try:\n",
    "    # project_root ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong cell 2\n",
    "    data_file_relative = \"data/btc_1d_data_2018_to_2025.csv\"  # ƒê·ªïi sang 4h n·∫øu mu·ªën: data/btc_4h_data_2018_to_2025.csv\n",
    "    DATA_PATH = str(project_root / data_file_relative)\n",
    "except NameError:\n",
    "    # Fallback n·∫øu project_root ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a (ch·∫°y cell n√†y tr∆∞·ªõc cell 2)\n",
    "    project_root = Path.cwd().parent\n",
    "    data_file_relative = \"data/btc_1d_data_2018_to_2025.csv\"\n",
    "    DATA_PATH = str(project_root / data_file_relative)\n",
    "\n",
    "LIMIT = 1500  # L·∫•y N d√≤ng cu·ªëi (<=0 = l·∫•y t·∫•t c·∫£)\n",
    "REFRESH_CACHE = False  # True = ƒë·ªçc l·∫°i CSV g·ªëc, False = d√πng cache normalized\n",
    "\n",
    "\n",
    "def infer_timeframe_from_filename(path: str) -> str:\n",
    "    \"\"\"Infer timeframe from filename.\"\"\"\n",
    "    name = path.lower()\n",
    "    if \"4h\" in name:\n",
    "        return \"4h\"\n",
    "    if \"1d\" in name:\n",
    "        return \"1d\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "TIMEFRAME = infer_timeframe_from_filename(DATA_PATH)  # Hi·ªÉn th·ªã timeframe d·ª±a v√†o t√™n file\n",
    "\n",
    "# Preprocessing parameters\n",
    "WINDOW_SIZE = 100  # S·ªë n·∫øn nh√¨n l·∫°i (sliding window)\n",
    "FEATURES = [\"close\"]  # Features s·ª≠ d·ª•ng\n",
    "\n",
    "# Model parameters\n",
    "LSTM_UNITS = [64, 32]  # S·ªë units cho m·ªói LSTM layer\n",
    "DROPOUT_RATE = 0.2  # Dropout rate\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 20  # S·ªë epochs\n",
    "BATCH_SIZE = 32  # Batch size\n",
    "EARLY_STOPPING_PATIENCE = 5  # S·ªë epochs ch·ªù tr∆∞·ªõc khi d·ª´ng\n",
    "\n",
    "# Print c·∫•u h√¨nh\n",
    "print(\"=\" * 60)\n",
    "print(\"‚öôÔ∏è  C·∫§U H√åNH\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Seed: {SEED}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Timeframe (from filename): {TIMEFRAME}\")\n",
    "print(f\"Limit (tail): {LIMIT}\")\n",
    "print(f\"Window size: {WINDOW_SIZE}\")\n",
    "print(f\"Features: {FEATURES}\")\n",
    "print(f\"LSTM units: {LSTM_UNITS}\")\n",
    "print(f\"Dropout: {DROPOUT_RATE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• B∆Ø·ªöC 1: ƒê·ªåC D·ªÆ LI·ªÜU CSV (LOCAL)\n",
    "\n",
    "### Gi·∫£i th√≠ch:\n",
    "- ƒê·ªçc d·ªØ li·ªáu gi√° t·ª´ file CSV local (m·∫∑c ƒë·ªãnh: `data/btc_1d_data_2018_to_2025.csv`)\n",
    "- Chu·∫©n ho√° v·ªÅ DataFrame v·ªõi: datetime, open, high, low, close, volume\n",
    "- Cache (optional) file CSV ƒë√£ chu·∫©n ho√° ƒë·ªÉ l·∫ßn sau ƒë·ªçc nhanh h∆°n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import fetch_binance_data\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu (CSV local)\n",
    "df = fetch_binance_data(\n",
    "    data_path=DATA_PATH,\n",
    "    timeframe=TIMEFRAME,\n",
    "    limit=LIMIT,\n",
    "    save_cache=not REFRESH_CACHE\n",
    ")\n",
    "\n",
    "# In 5 d√≤ng ƒë·∫ßu ti√™n\n",
    "print(\"\\nüìä 5 d√≤ng ƒë·∫ßu ti√™n c·ªßa d·ªØ li·ªáu:\")\n",
    "print(df.head())\n",
    "\n",
    "# Th·ªëng k√™ c∆° b·∫£n\n",
    "print(\"\\nüìä Th·ªëng k√™ d·ªØ li·ªáu:\")\n",
    "print(df.describe())\n",
    "\n",
    "# ƒê√°nh d·∫•u checklist\n",
    "print(\"\\n‚úÖ B∆∞·ªõc 1 ho√†n th√†nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V·∫Ω bi·ªÉu ƒë·ªì l·ªãch s·ª≠ gi√°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df['datetime'], df['close'], linewidth=2, color='#2E86AB', label='Gi√° ƒë√≥ng n·∫øn')\n",
    "plt.title(f'L·ªãch s·ª≠ gi√° Bitcoin ({TIMEFRAME})', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Th·ªùi gian', fontsize=12)\n",
    "plt.ylabel('Gi√° (USD)', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî® B∆Ø·ªöC 2: X·ª¨ L√ù D·ªÆ LI·ªÜU\n",
    "\n",
    "### Gi·∫£i th√≠ch:\n",
    "- **Scaling**: ƒê∆∞a d·ªØ li·ªáu v·ªÅ kho·∫£ng [0, 1] ƒë·ªÉ model h·ªçc t·ªët h∆°n\n",
    "- **Ch·ªëng data leakage**: scaler ƒë∆∞·ª£c **fit ch·ªâ tr√™n t·∫≠p train**, sau ƒë√≥ m·ªõi transform val/test\n",
    "- **Sliding Window**: T·∫°o sequences (60 ng√†y tr∆∞·ªõc ‚Üí d·ª± ƒëo√°n ng√†y ti·∫øp theo)\n",
    "- **Split Data**: Chia th√†nh train (80%), val (10%), test (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import prepare_data_for_lstm\n",
    "\n",
    "# Pipeline x·ª≠ l√Ω d·ªØ li·ªáu ho√†n ch·ªânh\n",
    "data_dict = prepare_data_for_lstm(\n",
    "    df=df,\n",
    "    features=FEATURES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    scaler_type='minmax'\n",
    ")\n",
    "\n",
    "# L·∫•y c√°c bi·∫øn\n",
    "X_train = data_dict['X_train']\n",
    "y_train = data_dict['y_train']\n",
    "X_val = data_dict['X_val']\n",
    "y_val = data_dict['y_val']\n",
    "X_test = data_dict['X_test']\n",
    "y_test = data_dict['y_test']\n",
    "scaler = data_dict['scaler']\n",
    "\n",
    "# ƒê√°nh d·∫•u checklist\n",
    "print(\"\\n‚úÖ B∆∞·ªõc 2 ho√†n th√†nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ki·ªÉm tra shapes c·ªßa d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä SHAPES C·ª¶A D·ªÆ LI·ªÜU\")\n",
    "print(\"=\"*60)\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}\")\n",
    "print(f\"y_val:   {y_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(f\"y_test:  {y_test.shape}\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† B∆Ø·ªöC 3: X√ÇY D·ª∞NG MODEL BiLSTM\n",
    "\n",
    "### Gi·∫£i th√≠ch:\n",
    "- **BiLSTM**: LSTM hai chi·ªÅu (nh√¨n c·∫£ qu√° kh·ª© v√† t∆∞∆°ng lai)\n",
    "- **Dropout**: B·ªè ng·∫´u nhi√™n neurons ƒë·ªÉ tr√°nh overfitting\n",
    "- **Dense layers**: K·∫øt h·ª£p features ƒë·ªÉ ƒë∆∞a ra d·ª± ƒëo√°n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import build_bilstm_model, print_model_summary\n",
    "\n",
    "# X√¢y d·ª±ng model\n",
    "input_shape = (WINDOW_SIZE, len(FEATURES))\n",
    "model = build_bilstm_model(\n",
    "    input_shape=input_shape,\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    dense_units=[16],\n",
    "    output_units=1\n",
    ")\n",
    "\n",
    "# In th√¥ng tin model\n",
    "print_model_summary(model)\n",
    "\n",
    "# ƒê√°nh d·∫•u checklist\n",
    "print(\"\\n‚úÖ B∆∞·ªõc 3 ho√†n th√†nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è B∆Ø·ªöC 4: TRAINING MODEL\n",
    "\n",
    "### Gi·∫£i th√≠ch:\n",
    "- **ModelCheckpoint**: L∆∞u l·∫°i model t·ªët nh·∫•t\n",
    "- **EarlyStopping**: D·ª´ng n·∫øu val_loss kh√¥ng gi·∫£m\n",
    "- **ReduceLROnPlateau**: Gi·∫£m learning rate n·∫øu kh√¥ng c·∫£i thi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import Config\n",
    "from src.training import train_model\n",
    "\n",
    "# T·∫°o Config object t·ª´ c√°c tham s·ªë c·ªßa notebook\n",
    "config = Config()\n",
    "config.training.epochs = EPOCHS\n",
    "config.training.batch_size = BATCH_SIZE\n",
    "config.training.early_stopping_patience = EARLY_STOPPING_PATIENCE\n",
    "\n",
    "# Training\n",
    "train_result = train_model(\n",
    "    model=model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# L·∫•y training history\n",
    "history = train_result['history']\n",
    "\n",
    "# Metadata training (ƒë·ªÉ ƒë∆∞a v√†o report)\n",
    "best_epoch = train_result.get('best_epoch')\n",
    "best_val_loss = train_result.get('best_val_loss')\n",
    "train_seconds = train_result.get('train_seconds')\n",
    "checkpoint_path_raw = train_result.get('checkpoint_path')\n",
    "checkpoint_path = str(checkpoint_path_raw) if checkpoint_path_raw is not None else None\n",
    "\n",
    "print(f\"\\nüìå Best epoch: {best_epoch}\")\n",
    "print(f\"üìå Best val_loss: {best_val_loss}\")\n",
    "print(f\"üìå Training time (s): {train_seconds}\")\n",
    "\n",
    "# ƒê√°nh d·∫•u checklist\n",
    "print(\"\\n‚úÖ B∆∞·ªõc 4 ho√†n th√†nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V·∫Ω bi·ªÉu ƒë·ªì training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization import plot_training_history\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä B∆Ø·ªöC 5: ƒê√ÅNH GI√Å & V·∫º BI·ªÇU ƒê·ªí\n",
    "\n",
    "### Gi·∫£i th√≠ch:\n",
    "- **MAE**: Sai s·ªë trung b√¨nh tuy·ªát ƒë·ªëi (USD)\n",
    "- **RMSE**: CƒÉn b·∫≠c 2 c·ªßa sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh (USD)\n",
    "- **MAPE**: Sai s·ªë ph·∫ßn trƒÉm trung b√¨nh (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import (\n",
    "    calculate_direction_accuracy,\n",
    "    evaluate_model,\n",
    "    print_sample_predictions\n",
    ")\n",
    "from src.visualization import plot_all_in_one, plot_predictions\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n test set\n",
    "eval_result = evaluate_model(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    scaler=scaler,\n",
    "    return_predictions=True\n",
    ")\n",
    "\n",
    "# L·∫•y d·ª± ƒëo√°n v√† gi√° tr·ªã th·∫≠t\n",
    "y_true = eval_result['y_true']\n",
    "y_pred = eval_result['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In m·ªôt s·ªë v√≠ d·ª• d·ª± ƒëo√°n\n",
    "print_sample_predictions(y_true, y_pred, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≠nh ƒë·ªô ch√≠nh x√°c xu h∆∞·ªõng\n",
    "direction_accuracy = calculate_direction_accuracy(y_true, y_pred)\n",
    "\n",
    "# L∆∞u v√†o eval_result ƒë·ªÉ report/metrics.json c√≥ th√™m th√¥ng tin\n",
    "eval_result['direction_accuracy'] = float(direction_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V·∫Ω bi·ªÉu ƒë·ªì predictions vs actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V·∫Ω bi·ªÉu ƒë·ªì t·ªïng h·ª£p (all-in-one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_in_one(history, y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ L∆ØU K·∫æT QU·∫¢\n",
    "\n",
    "T·∫•t c·∫£ k·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c `reports/notebook/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.results import (\n",
    "    create_results_folder,\n",
    "    save_config,\n",
    "    save_markdown_report,\n",
    "    save_metrics\n",
    ")\n",
    "from src.visualization import plot_all_in_one, plot_predictions, plot_training_history\n",
    "\n",
    "# T·∫°o folder k·∫øt qu·∫£ v·ªõi config ƒë·ªÉ ƒë·∫∑t t√™n chu·∫©n h√≥a\n",
    "folder_config = {\n",
    "    'timeframe': TIMEFRAME,\n",
    "    'window_size': WINDOW_SIZE\n",
    "}\n",
    "results_folder = create_results_folder(run_type=\"notebook\", config=folder_config)\n",
    "print(f\"üìÅ Folder k·∫øt qu·∫£: {results_folder}\\n\")\n",
    "\n",
    "# T·∫°o suffix cho t√™n file (l·∫•y timestamp t·ª´ t√™n folder: 2 ph·∫ßn cu·ªëi)\n",
    "folder_parts = results_folder.name.split('_')\n",
    "timestamp_suffix = '_'.join(folder_parts[-2:])  # L·∫•y 2 ph·∫ßn cu·ªëi (YYYYMMDD_HHMMSS)\n",
    "\n",
    "# V·∫Ω v√† l∆∞u bi·ªÉu ƒë·ªì\n",
    "plot_history_file = results_folder / f\"training_history_{timestamp_suffix}.png\"\n",
    "plot_predictions_file = results_folder / f\"predictions_{timestamp_suffix}.png\"\n",
    "plot_all_in_one_file = results_folder / f\"all_in_one_{timestamp_suffix}.png\"\n",
    "\n",
    "plot_training_history(history, save_path=str(plot_history_file))\n",
    "plot_predictions(y_true, y_pred, save_path=str(plot_predictions_file))\n",
    "plot_all_in_one(history, y_true, y_pred, save_path=str(plot_all_in_one_file))\n",
    "\n",
    "# Metadata d·ªØ li·ªáu\n",
    "try:\n",
    "    data_rows = len(df)\n",
    "    # Polars DataFrame: s·ª≠ d·ª•ng .row() ho·∫∑c .select()\n",
    "    data_start = str(df.select('datetime').row(0)[0])\n",
    "    data_end = str(df.select('datetime').row(-1)[0])\n",
    "except Exception:\n",
    "    try:\n",
    "        # Fallback: th·ª≠ c√°ch kh√°c\n",
    "        data_start = str(df['datetime'].min())\n",
    "        data_end = str(df['datetime'].max())\n",
    "    except Exception:\n",
    "        data_rows = None\n",
    "        data_start = None\n",
    "        data_end = None\n",
    "\n",
    "# Metadata split\n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_val)\n",
    "test_samples = len(X_test)\n",
    "scaler_type = 'minmax'\n",
    "\n",
    "# Chu·∫©n b·ªã config v√† metrics\n",
    "config_dict = {\n",
    "    'data_path': DATA_PATH,  # Th√™m data_path ƒë·ªÉ hi·ªÉn th·ªã Source CSV\n",
    "    'symbol': SYMBOL,\n",
    "    'timeframe': TIMEFRAME,\n",
    "    'limit': LIMIT,\n",
    "    'data_rows': data_rows,\n",
    "    'data_start': data_start,\n",
    "    'data_end': data_end,\n",
    "    'window_size': WINDOW_SIZE,\n",
    "    'features': FEATURES,\n",
    "    'scaler_type': scaler_type,\n",
    "    'train_samples': train_samples,\n",
    "    'val_samples': val_samples,\n",
    "    'test_samples': test_samples,\n",
    "    'seed': SEED,\n",
    "    'lstm_units': LSTM_UNITS,\n",
    "    'dropout_rate': DROPOUT_RATE,\n",
    "    'epochs': EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'early_stopping_patience': EARLY_STOPPING_PATIENCE,\n",
    "    'learning_rate': config.training.learning_rate,\n",
    "    'intra_threads': config.runtime.intra_op_threads,\n",
    "    'best_epoch': best_epoch,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'train_seconds': train_seconds,\n",
    "    'checkpoint_path': checkpoint_path,\n",
    "}\n",
    "\n",
    "plots_dict = {\n",
    "    'training_history': f\"training_history_{timestamp_suffix}.png\",\n",
    "    'predictions': f\"predictions_{timestamp_suffix}.png\",\n",
    "    'all_in_one': f\"all_in_one_{timestamp_suffix}.png\"\n",
    "}\n",
    "\n",
    "# L∆∞u b√°o c√°o\n",
    "save_markdown_report(\n",
    "    folder_path=results_folder,\n",
    "    config=config_dict,\n",
    "    metrics=eval_result,\n",
    "    history=history.history,\n",
    "    plots=plots_dict\n",
    ")\n",
    "save_config(results_folder, config_dict)\n",
    "save_metrics(results_folder, eval_result)\n",
    "\n",
    "# ƒê√°nh d·∫•u checklist\n",
    "print(\"\\n‚úÖ B∆∞·ªõc 5 ho√†n th√†nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ HO√ÄN TH√ÄNH!\n",
    "\n",
    "### Checklist:\n",
    "- [x] B∆∞·ªõc 1: L·∫•y d·ªØ li·ªáu t·ª´ Binance\n",
    "- [x] B∆∞·ªõc 2: X·ª≠ l√Ω d·ªØ li·ªáu\n",
    "- [x] B∆∞·ªõc 3: X√¢y d·ª±ng model BiLSTM\n",
    "- [x] B∆∞·ªõc 4: Training model\n",
    "- [x] B∆∞·ªõc 5: ƒê√°nh gi√° & V·∫Ω bi·ªÉu ƒë·ªì\n",
    "\n",
    "### K·∫øt qu·∫£:\n",
    "- B√°o c√°o Markdown: `reports/notebook/BiLSTM_YYYYMMDD_HHMMSS/results_*.md`\n",
    "- Bi·ªÉu ƒë·ªì: C√°c file PNG trong c√πng folder\n",
    "- Config & Metrics: File JSON trong c√πng folder\n",
    "\n",
    "### Ti·∫øp theo:\n",
    "- Th·ª≠ thay ƒë·ªïi c√°c tham s·ªë (window size, LSTM units, v.v.)\n",
    "- Th√™m c√°c features kh√°c (volume, open, high, low)\n",
    "- Th·ª≠ timeframe kh√°c (4h, 1h)\n",
    "\n",
    "**Ch√∫c b·∫°n th√†nh c√¥ng! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
