{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ M√¥ H√¨nh D·ª± B√°o Gi√° Bitcoin V·ªõi BiLSTM\n",
    "\n",
    "Notebook n√†y h∆∞·ªõng d·∫´n b·∫°n t·ª´ng b∆∞·ªõc ƒë·ªÉ x√¢y d·ª±ng v√† hu·∫•n luy·ªán m√¥ h√¨nh d·ª± b√°o gi√° Bitcoin.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Checklist\n",
    "\n",
    "- [ ] B∆∞·ªõc 1: L·∫•y d·ªØ li·ªáu t·ª´ Binance\n",
    "- [ ] B∆∞·ªõc 2: X·ª≠ l√Ω d·ªØ li·ªáu\n",
    "- [ ] B∆∞·ªõc 3: X√¢y d·ª±ng model BiLSTM\n",
    "- [ ] B∆∞·ªõc 4: Training model\n",
    "- [ ] B∆∞·ªõc 5: ƒê√°nh gi√° & V·∫Ω bi·ªÉu ƒë·ªì\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Tips\n",
    "\n",
    "1. Ch·∫°y t·ª´ng cell theo th·ª© t·ª± t·ª´ tr√™n xu·ªëng d∆∞·ªõi\n",
    "2. ƒê·ªçc comments trong code ƒë·ªÉ hi·ªÉu\n",
    "3. N·∫øu c·∫ßn gi·∫£i th√≠ch chi ti·∫øt, xem `docs/` folder\n",
    "4. Ngh·ªâ gi·∫£i lao n·∫øu c·∫£m th·∫•y ng·ª£p!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 0. Setup & C·∫•u H√¨nh\n",
    "\n",
    "C·∫•u h√¨nh TensorFlow v√† import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress TensorFlow warnings BEFORE importing TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Ch·ªâ hi·ªÉn th·ªã ERROR\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # T·∫Øt oneDNN warnings\n",
    "\n",
    "# Suppress Python warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', message='.*np.object.*')\n",
    "warnings.filterwarnings('ignore', message='.*oneDNN.*')\n",
    "warnings.filterwarnings('ignore', message='.*CUDA.*')\n",
    "warnings.filterwarnings('ignore', message='.*Could not find cuda.*')\n",
    "warnings.filterwarnings('ignore', message='.*cuda drivers.*')\n",
    "warnings.filterwarnings('ignore', message='.*GPU will not be used.*')\n",
    "\n",
    "# Third-party imports (must be after env vars for TensorFlow)\n",
    "import numpy as np  # noqa: E402\n",
    "import polars as pl  # noqa: E402\n",
    "\n",
    "# Import v√† suppress TensorFlow logger ngay l·∫≠p t·ª©c\n",
    "try:\n",
    "    import tensorflow as tf  # noqa: E402\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    # Suppress stderr output t·ª´ TensorFlow\n",
    "    logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "except ImportError:\n",
    "    pass  # TensorFlow ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
    "\n",
    "# Setup project path (notebook runs from notebooks/, need to go up one level)\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Local imports\n",
    "from src.runtime import (  # noqa: E402\n",
    "    configure_tensorflow_runtime,\n",
    "    print_tensorflow_info,\n",
    "    set_random_seed\n",
    ")\n",
    "\n",
    "# C·∫•u h√¨nh TensorFlow cho CPU AMD\n",
    "configure_tensorflow_runtime(\n",
    "    intra_op_threads=12,\n",
    "    inter_op_threads=2,\n",
    "    enable_xla=True\n",
    ")\n",
    "\n",
    "# In th√¥ng tin TensorFlow\n",
    "print_tensorflow_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 1. Ch·ªçn Preset\n",
    "\n",
    "Ch·ªçn m·ªôt trong c√°c preset c√≥ s·∫µn ho·∫∑c c·∫•u h√¨nh th·ªß c√¥ng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CH·ªåN PRESET ====================\n",
    "# Ch·ªçn preset t·ª´ danh s√°ch b√™n d∆∞·ªõi, ho·∫∑c set PRESET_NAME = None ƒë·ªÉ c·∫•u h√¨nh th·ªß c√¥ng\n",
    "#\n",
    "# Preset Scalping (15m - si√™u ng·∫Øn h·∫°n):\n",
    "# - 'scalping-ultra-fast': 10k d√≤ng, window=24 (6h), epochs=5, lstm=[16] - Scalping c·ª±c nhanh\n",
    "# - 'scalping-fast': 20k d√≤ng, window=48 (12h), epochs=10, lstm=[32,16] - Scalping nhanh\n",
    "#\n",
    "# Preset Intraday (15m - ng·∫Øn h·∫°n):\n",
    "# - 'intraday-light': 30k d√≤ng, window=96 (1d), epochs=15, lstm=[32,16] - Intraday nh·∫π\n",
    "# - 'intraday-balanced': 50k d√≤ng, window=144 (1.5d), epochs=25, lstm=[64,32] - Intraday c√¢n b·∫±ng\n",
    "#\n",
    "# Preset Swing (15m - trung h·∫°n):\n",
    "# - 'swing-fast': 70k d√≤ng, window=240 (2.5d), epochs=30, lstm=[64,32] - Swing nhanh\n",
    "# - 'swing-balanced': 100k d√≤ng, window=384 (4d), epochs=50, lstm=[128,64,32] - Swing c√¢n b·∫±ng\n",
    "#\n",
    "# Preset Long-term & Production (15m):\n",
    "# - 'long-term': 150k d√≤ng, window=576 (6d), epochs=80, lstm=[256,128,64,32] - D·ª± ƒëo√°n d√†i h·∫°n\n",
    "# - 'production': 200k d√≤ng, window=768 (8d), epochs=100, lstm=[256,128,64,32] - Production ch·∫•t l∆∞·ª£ng cao nh·∫•t\n",
    "#\n",
    "# Preset Legacy (cho c√°c timeframe kh√°c):\n",
    "# - 'default': 50k d√≤ng 15m (default config)\n",
    "# - 'fast': 20k d√≤ng 15m (test nhanh)\n",
    "# - '1h-light': 10k d√≤ng 1h\n",
    "# - '4h-balanced': 2k d√≤ng 4h\n",
    "#\n",
    "# Set PRESET_NAME = None ƒë·ªÉ d√πng c·∫•u h√¨nh th·ªß c√¥ng ·ªü cell b√™n d∆∞·ªõi\n",
    "\n",
    "PRESET_NAME = 'fast'  # ƒê·ªïi t√™n preset ·ªü ƒë√¢y, ho·∫∑c None ƒë·ªÉ c·∫•u h√¨nh th·ªß c√¥ng\n",
    "\n",
    "# Import preset functions\n",
    "from src import (  # noqa: E402\n",
    "    Config,\n",
    "    # Scalping presets\n",
    "    get_scalping_ultra_fast_config,\n",
    "    get_scalping_fast_config,\n",
    "    # Intraday presets\n",
    "    get_intraday_light_config,\n",
    "    get_intraday_balanced_config,\n",
    "    # Swing presets\n",
    "    get_swing_fast_config,\n",
    "    get_swing_balanced_config,\n",
    "    # Long-term & Production presets\n",
    "    get_long_term_config,\n",
    "    get_production_config,\n",
    "    # Legacy presets\n",
    "    get_default_config,\n",
    "    get_fast_config,\n",
    "    get_1h_light_config,\n",
    "    get_4h_balanced_config,\n",
    ")\n",
    "\n",
    "# Map preset name v·ªõi function\n",
    "PRESET_MAP = {\n",
    "    # Scalping presets\n",
    "    'scalping-ultra-fast': get_scalping_ultra_fast_config,\n",
    "    'scalping-fast': get_scalping_fast_config,\n",
    "    # Intraday presets\n",
    "    'intraday-light': get_intraday_light_config,\n",
    "    'intraday-balanced': get_intraday_balanced_config,\n",
    "    # Swing presets\n",
    "    'swing-fast': get_swing_fast_config,\n",
    "    'swing-balanced': get_swing_balanced_config,\n",
    "    # Long-term & Production presets\n",
    "    'long-term': get_long_term_config,\n",
    "    'production': get_production_config,\n",
    "    # Legacy presets\n",
    "    'default': get_default_config,\n",
    "    'fast': get_fast_config,\n",
    "    '1h-light': get_1h_light_config,\n",
    "    '4h-balanced': get_4h_balanced_config,\n",
    "}\n",
    "\n",
    "# Load preset ho·∫∑c t·∫°o config r·ªóng\n",
    "if PRESET_NAME and PRESET_NAME in PRESET_MAP:\n",
    "    config = PRESET_MAP[PRESET_NAME]()\n",
    "    print(f\"üì¶ ƒê√£ ch·ªçn preset: {PRESET_NAME}\")\n",
    "    print(f\"   Limit: {config.data.limit}, Window: {config.preprocessing.window_size}, Epochs: {config.training.epochs}\")\n",
    "    print(f\"   LSTM units: {config.model.lstm_units}\\n\")\n",
    "else:\n",
    "    config = Config()\n",
    "    if PRESET_NAME:\n",
    "        print(f\"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y preset '{PRESET_NAME}', d√πng c·∫•u h√¨nh m·∫∑c ƒë·ªãnh.\\n\")\n",
    "    else:\n",
    "        print(\"üîß Kh√¥ng ch·ªçn preset, s·∫Ω d√πng c·∫•u h√¨nh th·ªß c√¥ng ·ªü cell b√™n d∆∞·ªõi.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T√πy ch·ªânh th√™m (Ghi ƒë√® preset ho·∫∑c c·∫•u h√¨nh th·ªß c√¥ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== C·∫§U H√åNH TH·ª¶ C√îNG / GHI ƒê·ªÄ PRESET ====================\n",
    "\n",
    "# NOTE: N·∫øu ƒë√£ ch·ªçn preset ·ªü cell tr√™n, c√°c gi√° tr·ªã None s·∫Ω d√πng preset.\n",
    "#       Set gi√° tr·ªã c·ª• th·ªÉ ƒë·ªÉ ghi ƒë√® preset.\n",
    "\n",
    "# Reproducibility\n",
    "SEED = config.runtime.seed  # ƒê·ªïi seed ƒë·ªÉ ch·∫°y th·ª≠ nghi·ªám; ƒë·∫∑t <0 ƒë·ªÉ kh√¥ng c·ªë ƒë·ªãnh\n",
    "\n",
    "# Symbol\n",
    "SYMBOL = \"BTC/USDT\"  # Trading pair symbol\n",
    "\n",
    "# Data parameters (CSV local)\n",
    "# S·ª≠ d·ª•ng project_root ƒë·ªÉ t√¨m ƒë∆∞·ªùng d·∫´n ƒë√∫ng (notebook ch·∫°y t·ª´ notebooks/)\n",
    "try:\n",
    "    # project_root ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong cell 2\n",
    "    data_file_relative = \"data/btc_1d_data_2018_to_2025.csv\"  # ƒê·ªïi sang 4h/1h/15m: btc_4h_data_2018_to_2025.csv, btc_1h_data_2018_to_2025.csv, btc_15m_data_2018_to_2025.csv\n",
    "    DATA_PATH = str(project_root / data_file_relative)\n",
    "except NameError:\n",
    "    # Fallback n·∫øu project_root ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a (ch·∫°y cell n√†y tr∆∞·ªõc cell 2)\n",
    "    project_root = Path.cwd().parent\n",
    "    data_file_relative = \"data/btc_1d_data_2018_to_2025.csv\"\n",
    "    DATA_PATH = str(project_root / data_file_relative)\n",
    "\n",
    "LIMIT = None  # L·∫•y N d√≤ng cu·ªëi (<=0 = l·∫•y t·∫•t c·∫£), None = d√πng preset\n",
    "REFRESH_CACHE = False  # True = ƒë·ªçc l·∫°i CSV g·ªëc, False = d√πng cache normalized\n",
    "\n",
    "\n",
    "def infer_timeframe_from_filename(path: str) -> str:\n",
    "    \"\"\"Infer timeframe from filename.\"\"\"\n",
    "    name = path.lower()\n",
    "    # Th·ª≠ theo th·ª© t·ª± t·ª´ d√†i ƒë·∫øn ng·∫Øn ƒë·ªÉ tr√°nh ghi ƒë√® kh√¥ng ch√≠nh x√°c\n",
    "    if \"15m\" in name:\n",
    "        return \"15m\"\n",
    "    if \"1h\" in name:\n",
    "        return \"1h\"\n",
    "    if \"4h\" in name:\n",
    "        return \"4h\"\n",
    "    if \"1d\" in name:\n",
    "        return \"1d\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "TIMEFRAME = infer_timeframe_from_filename(DATA_PATH)  # Hi·ªÉn th·ªã timeframe d·ª±a v√†o t√™n file\n",
    "\n",
    "# Preprocessing parameters (None = d√πng preset)\n",
    "WINDOW_SIZE = None  # S·ªë n·∫øn nh√¨n l·∫°i (sliding window)\n",
    "FEATURES = [\"close\"]  # Features s·ª≠ d·ª•ng\n",
    "\n",
    "# Model parameters (None = d√πng preset)\n",
    "LSTM_UNITS = None  # S·ªë units cho m·ªói LSTM layer\n",
    "DROPOUT_RATE = None  # Dropout rate\n",
    "\n",
    "# Training parameters (None = d√πng preset)\n",
    "EPOCHS = None  # S·ªë epochs\n",
    "BATCH_SIZE = None  # Batch size\n",
    "EARLY_STOPPING_PATIENCE = None  # S·ªë epochs ch·ªù tr∆∞·ªõc khi d·ª´ng\n",
    "\n",
    "# Ghi ƒë√® config v·ªõi c√°c gi√° tr·ªã ƒë√£ set\n",
    "if SEED is not None:\n",
    "    config.runtime.seed = SEED\n",
    "if LIMIT is not None:\n",
    "    config.data.limit = LIMIT\n",
    "if TIMEFRAME:\n",
    "    config.data.timeframe = TIMEFRAME\n",
    "if DATA_PATH:\n",
    "    config.data.data_path = DATA_PATH\n",
    "if REFRESH_CACHE is not None:\n",
    "    config.data.refresh_cache = REFRESH_CACHE\n",
    "if FEATURES is not None:\n",
    "    config.data.features = FEATURES\n",
    "if WINDOW_SIZE is not None:\n",
    "    config.preprocessing.window_size = WINDOW_SIZE\n",
    "if LSTM_UNITS is not None:\n",
    "    config.model.lstm_units = LSTM_UNITS\n",
    "if DROPOUT_RATE is not None:\n",
    "    config.model.dropout_rate = DROPOUT_RATE\n",
    "if EPOCHS is not None:\n",
    "    config.training.epochs = EPOCHS\n",
    "if BATCH_SIZE is not None:\n",
    "    config.training.batch_size = BATCH_SIZE\n",
    "if EARLY_STOPPING_PATIENCE is not None:\n",
    "    config.training.early_stopping_patience = EARLY_STOPPING_PATIENCE\n",
    "\n",
    "# C·ªë ƒë·ªãnh seed ƒë·ªÉ t√°i l·∫≠p k·∫øt qu·∫£ (n·∫øu SEED < 0 th√¨ b·ªè qua)\n",
    "if config.runtime.seed is not None and config.runtime.seed >= 0:\n",
    "    set_random_seed(config.runtime.seed, deterministic=True)\n",
    "\n",
    "# Print c·∫•u h√¨nh\n",
    "print(\"=\" * 60)\n",
    "print(\"‚öôÔ∏è  C·∫§U H√åNH CU·ªêI C√ôNG\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Seed: {config.runtime.seed}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Timeframe: {TIMEFRAME}\")\n",
    "print(f\"Limit: {config.data.limit}\")\n",
    "print(f\"Window size: {config.preprocessing.window_size}\")\n",
    "print(f\"Features: {config.data.features}\")\n",
    "print(f\"LSTM units: {config.model.lstm_units}\")\n",
    "print(f\"Dropout: {config.model.dropout_rate}\")\n",
    "print(f\"Epochs: {config.training.epochs}\")\n",
    "print(f\"Batch size: {config.training.batch_size}\")\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• B∆Ø·ªöC 1: ƒê·ªåC D·ªÆ LI·ªÜU CSV (LOCAL)\n",
    "\n",
    "### Gi·∫£i th√≠ch:\n",
    "- ƒê·ªçc d·ªØ li·ªáu gi√° t·ª´ file CSV local (m·∫∑c ƒë·ªãnh: `data/btc_1d_data_2018_to_2025.csv`)\n",
    "- Chu·∫©n ho√° v·ªÅ DataFrame v·ªõi: datetime, open, high, low, close, volume\n",
    "- Cache (optional) file CSV ƒë√£ chu·∫©n ho√° ƒë·ªÉ l·∫ßn sau ƒë·ªçc nhanh h∆°n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import fetch_binance_data  # noqa: E402\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu (CSV local)\n",
    "df = fetch_binance_data(\n",
    "    data_path=DATA_PATH,\n",
    "    timeframe=TIMEFRAME,\n",
    "    limit=config.data.limit,\n",
    "    save_cache=not REFRESH_CACHE\n",
    ")\n",
    "\n",
    "# In 5 d√≤ng ƒë·∫ßu ti√™n\n",
    "print(\"\\nüìä 5 d√≤ng ƒë·∫ßu ti√™n c·ªßa d·ªØ li·ªáu:\")\n",
    "print(df.head())\n",
    "\n",
    "# Th·ªëng k√™ c∆° b·∫£n\n",
    "print(\"\\nüìä Th·ªëng k√™ d·ªØ li·ªáu:\")\n",
    "print(df.describe())\n",
    "\n",
    "# ƒê√°nh d·∫•u checklist\n",
    "print(\"\\n‚úÖ B∆∞·ªõc 1 ho√†n th√†nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V·∫Ω bi·ªÉu ƒë·ªì l·ªãch s·ª≠ gi√°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df['datetime'], df['close'], linewidth=2, color='#2E86AB', label='Gi√° ƒë√≥ng n·∫øn')\n",
    "plt.title(f'L·ªãch s·ª≠ gi√° Bitcoin ({TIMEFRAME})', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Th·ªùi gian', fontsize=12)\n",
    "plt.ylabel('Gi√° (USD)', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî® B∆Ø·ªöC 2: X·ª¨ L√ù D·ªÆ LI·ªÜU\n",
    "\n",
    "### Gi·∫£i th√≠ch:\n",
    "- **Scaling**: ƒê∆∞a d·ªØ li·ªáu v·ªÅ kho·∫£ng [0, 1] ƒë·ªÉ model h·ªçc t·ªët h∆°n\n",
    "- **Ch·ªëng data leakage**: scaler ƒë∆∞·ª£c **fit ch·ªâ tr√™n t·∫≠p train**, sau ƒë√≥ m·ªõi transform val/test\n",
    "- **Sliding Window**: T·∫°o sequences (60 ng√†y tr∆∞·ªõc ‚Üí d·ª± ƒëo√°n ng√†y ti·∫øp theo)\n",
    "- **Split Data**: Chia th√†nh train (80%), val (10%), test (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import prepare_data_for_lstm  # noqa: E402\n",
    "\n",
    "# Pipeline x·ª≠ l√Ω d·ªØ li·ªáu ho√†n ch·ªânh\n",
    "data_dict = prepare_data_for_lstm(\n",
    "    df=df,\n",
    "    features=config.data.features,\n",
    "    window_size=config.preprocessing.window_size,\n",
    "    scaler_type='minmax'\n",
    ")\n",
    "\n",
    "# L·∫•y c√°c bi·∫øn\n",
    "X_train = data_dict['X_train']\n",
    "y_train = data_dict['y_train']\n",
    "X_val = data_dict['X_val']\n",
    "y_val = data_dict['y_val']\n",
    "X_test = data_dict['X_test']\n",
    "y_test = data_dict['y_test']\n",
    "scaler = data_dict['scaler']\n",
    "\n",
    "# ƒê√°nh d·∫•u checklist\n",
    "print(\"\\n‚úÖ B∆∞·ªõc 2 ho√†n th√†nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ki·ªÉm tra shapes c·ªßa d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä SHAPES C·ª¶A D·ªÆ LI·ªÜU\")\n",
    "print(\"=\"*60)\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}\")\n",
    "print(f\"y_val:   {y_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(f\"y_test:  {y_test.shape}\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† B∆Ø·ªöC 3: X√ÇY D·ª∞NG MODEL BiLSTM\n",
    "\n",
    "### Gi·∫£i th√≠ch:\n",
    "- **BiLSTM**: LSTM hai chi·ªÅu (nh√¨n c·∫£ qu√° kh·ª© v√† t∆∞∆°ng lai)\n",
    "- **Dropout**: B·ªè ng·∫´u nhi√™n neurons ƒë·ªÉ tr√°nh overfitting\n",
    "- **Dense layers**: K·∫øt h·ª£p features ƒë·ªÉ ƒë∆∞a ra d·ª± ƒëo√°n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import build_bilstm_model, print_model_summary  # noqa: E402\n",
    "\n",
    "# X√¢y d·ª±ng model\n",
    "input_shape = (config.preprocessing.window_size, len(config.data.features))\n",
    "model = build_bilstm_model(\n",
    "    input_shape=input_shape,\n",
    "    lstm_units=config.model.lstm_units,\n",
    "    dropout_rate=config.model.dropout_rate,\n",
    "    dense_units=config.model.dense_units,\n",
    "    output_units=1\n",
    ")\n",
    "\n",
    "# In th√¥ng tin model\n",
    "print_model_summary(model)\n",
    "\n",
    "# ƒê√°nh d·∫•u checklist\n",
    "print(\"\\n‚úÖ B∆∞·ªõc 3 ho√†n th√†nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è B∆Ø·ªöC 4: TRAINING MODEL\n",
    "\n",
    "### Gi·∫£i th√≠ch:\n",
    "- **ModelCheckpoint**: L∆∞u l·∫°i model t·ªët nh·∫•t\n",
    "- **EarlyStopping**: D·ª´ng n·∫øu val_loss kh√¥ng gi·∫£m\n",
    "- **ReduceLROnPlateau**: Gi·∫£m learning rate n·∫øu kh√¥ng c·∫£i thi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import train_model  # noqa: E402\n",
    "\n",
    "# Training\n",
    "train_result = train_model(\n",
    "    model=model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# L·∫•y training history\n",
    "history = train_result['history']\n",
    "\n",
    "# Metadata training (ƒë·ªÉ ƒë∆∞a v√†o report)\n",
    "best_epoch = train_result.get('best_epoch')\n",
    "best_val_loss = train_result.get('best_val_loss')\n",
    "train_seconds = train_result.get('train_seconds')\n",
    "checkpoint_path_raw = train_result.get('checkpoint_path')\n",
    "checkpoint_path = str(checkpoint_path_raw) if checkpoint_path_raw is not None else None\n",
    "\n",
    "print(f\"\\nüìå Best epoch: {best_epoch}\")\n",
    "print(f\"üìå Best val_loss: {best_val_loss}\")\n",
    "print(f\"üìå Training time (s): {train_seconds}\")\n",
    "\n",
    "# ƒê√°nh d·∫•u checklist\n",
    "print(\"\\n‚úÖ B∆∞·ªõc 4 ho√†n th√†nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V·∫Ω bi·ªÉu ƒë·ªì training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization import plot_training_history  # noqa: E402\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä B∆Ø·ªöC 5: ƒê√ÅNH GI√Å & V·∫º BI·ªÇU ƒê·ªí\n",
    "\n",
    "### Gi·∫£i th√≠ch:\n",
    "- **MAE**: Sai s·ªë trung b√¨nh tuy·ªát ƒë·ªëi (USD)\n",
    "- **RMSE**: CƒÉn b·∫≠c 2 c·ªßa sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh (USD)\n",
    "- **MAPE**: Sai s·ªë ph·∫ßn trƒÉm trung b√¨nh (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import (  # noqa: E402\n",
    "    calculate_direction_accuracy,\n",
    "    evaluate_model,\n",
    "    print_sample_predictions\n",
    ")\n",
    "from src.visualization import plot_all_in_one, plot_predictions  # noqa: E402\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n test set\n",
    "eval_result = evaluate_model(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    scaler=scaler,\n",
    "    return_predictions=True\n",
    ")\n",
    "\n",
    "# L·∫•y d·ª± ƒëo√°n v√† gi√° tr·ªã th·∫≠t\n",
    "y_true = eval_result['y_true']\n",
    "y_pred = eval_result['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In m·ªôt s·ªë v√≠ d·ª• d·ª± ƒëo√°n\n",
    "print_sample_predictions(y_true, y_pred, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≠nh ƒë·ªô ch√≠nh x√°c xu h∆∞·ªõng\n",
    "direction_accuracy = calculate_direction_accuracy(y_true, y_pred)\n",
    "\n",
    "# L∆∞u v√†o eval_result ƒë·ªÉ report/metrics.json c√≥ th√™m th√¥ng tin\n",
    "eval_result['direction_accuracy'] = float(direction_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V·∫Ω bi·ªÉu ƒë·ªì predictions vs actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V·∫Ω bi·ªÉu ƒë·ªì t·ªïng h·ª£p (all-in-one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_in_one(history, y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ L∆ØU K·∫æT QU·∫¢\n",
    "\n",
    "T·∫•t c·∫£ k·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c `reports/notebook/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.results import (  # noqa: E402\n",
    "    create_results_folder,\n",
    "    save_config,\n",
    "    save_markdown_report,\n",
    "    save_metrics\n",
    ")\n",
    "\n",
    "# T·∫°o folder k·∫øt qu·∫£ v·ªõi config ƒë·ªÉ ƒë·∫∑t t√™n chu·∫©n h√≥a\n",
    "folder_config = {\n",
    "    'timeframe': TIMEFRAME,\n",
    "    'window_size': config.preprocessing.window_size\n",
    "}\n",
    "results_folder = create_results_folder(run_type=\"notebook\", config=folder_config)\n",
    "print(f\"üìÅ Folder k·∫øt qu·∫£: {results_folder}\\n\")\n",
    "\n",
    "# T·∫°o suffix cho t√™n file (l·∫•y timestamp t·ª´ t√™n folder: 2 ph·∫ßn cu·ªëi)\n",
    "folder_parts = results_folder.name.split('_')\n",
    "timestamp_suffix = '_'.join(folder_parts[-2:])  # L·∫•y 2 ph·∫ßn cu·ªëi (YYYYMMDD_HHMMSS)\n",
    "\n",
    "# V·∫Ω v√† l∆∞u bi·ªÉu ƒë·ªì\n",
    "plot_history_file = results_folder / f\"training_history_{timestamp_suffix}.png\"\n",
    "plot_predictions_file = results_folder / f\"predictions_{timestamp_suffix}.png\"\n",
    "plot_all_in_one_file = results_folder / f\"all_in_one_{timestamp_suffix}.png\"\n",
    "\n",
    "plot_training_history(history, save_path=str(plot_history_file))\n",
    "plot_predictions(y_true, y_pred, save_path=str(plot_predictions_file))\n",
    "plot_all_in_one(history, y_true, y_pred, save_path=str(plot_all_in_one_file))\n",
    "\n",
    "# Metadata d·ªØ li·ªáu\n",
    "try:\n",
    "    data_rows = len(df)\n",
    "    data_start = str(df.select('datetime').row(0)[0])\n",
    "    data_end = str(df.select('datetime').row(-1)[0])\n",
    "except Exception:\n",
    "    data_rows = None\n",
    "    data_start = None\n",
    "    data_end = None\n",
    "\n",
    "# Metadata split\n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_val)\n",
    "test_samples = len(X_test)\n",
    "scaler_type = 'minmax'\n",
    "\n",
    "# Chu·∫©n b·ªã config v√† metrics\n",
    "config_dict = {\n",
    "    'data_path': DATA_PATH,\n",
    "    'symbol': SYMBOL,\n",
    "    'timeframe': TIMEFRAME,\n",
    "    'limit': config.data.limit,\n",
    "    'data_rows': data_rows,\n",
    "    'data_start': data_start,\n",
    "    'data_end': data_end,\n",
    "    'window_size': config.preprocessing.window_size,\n",
    "    'features': config.data.features,\n",
    "    'scaler_type': scaler_type,\n",
    "    'train_samples': train_samples,\n",
    "    'val_samples': val_samples,\n",
    "    'test_samples': test_samples,\n",
    "    'seed': config.runtime.seed,\n",
    "    'lstm_units': config.model.lstm_units,\n",
    "    'dropout_rate': config.model.dropout_rate,\n",
    "    'epochs': config.training.epochs,\n",
    "    'batch_size': config.training.batch_size,\n",
    "    'early_stopping_patience': config.training.early_stopping_patience,\n",
    "    'learning_rate': config.training.learning_rate,\n",
    "    'intra_threads': config.runtime.intra_op_threads,\n",
    "    'best_epoch': best_epoch,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'train_seconds': train_seconds,\n",
    "    'checkpoint_path': checkpoint_path,\n",
    "}\n",
    "\n",
    "plots_dict = {\n",
    "    'training_history': f\"training_history_{timestamp_suffix}.png\",\n",
    "    'predictions': f\"predictions_{timestamp_suffix}.png\",\n",
    "    'all_in_one': f\"all_in_one_{timestamp_suffix}.png\"\n",
    "}\n",
    "\n",
    "# L∆∞u b√°o c√°o\n",
    "save_markdown_report(\n",
    "    folder_path=results_folder,\n",
    "    config=config_dict,\n",
    "    metrics=eval_result,\n",
    "    history=history.history,\n",
    "    plots=plots_dict\n",
    ")\n",
    "save_config(results_folder, config_dict)\n",
    "save_metrics(results_folder, eval_result)\n",
    "\n",
    "# ƒê√°nh d·∫•u checklist\n",
    "print(\"\\n‚úÖ B∆∞·ªõc 5 ho√†n th√†nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ HO√ÄN TH√ÄNH!\n",
    "\n",
    "### Checklist:\n",
    "- [x] B∆∞·ªõc 1: L·∫•y d·ªØ li·ªáu t·ª´ Binance\n",
    "- [x] B∆∞·ªõc 2: X·ª≠ l√Ω d·ªØ li·ªáu\n",
    "- [x] B∆∞·ªõc 3: X√¢y d·ª±ng model BiLSTM\n",
    "- [x] B∆∞·ªõc 4: Training model\n",
    "- [x] B∆∞·ªõc 5: ƒê√°nh gi√° & V·∫Ω bi·ªÉu ƒë·ªì\n",
    "\n",
    "### K·∫øt qu·∫£:\n",
    "- B√°o c√°o Markdown: `reports/notebook/BiLSTM_YYYYMMDD_HHMMSS/results_*.md`\n",
    "- Bi·ªÉu ƒë·ªì: C√°c file PNG trong c√πng folder\n",
    "- Config & Metrics: File JSON trong c√πng folder\n",
    "\n",
    "### Ti·∫øp theo:\n",
    "- Th·ª≠ thay ƒë·ªïi preset:\n",
    "  - **Scalping (15m):** scalping-ultra-fast, scalping-fast\n",
    "  - **Intraday (15m):** intraday-light, intraday-balanced\n",
    "  - **Swing (15m):** swing-fast, swing-balanced\n",
    "  - **Long-term (15m):** long-term\n",
    "  - **Production (15m):** production\n",
    "  - **Legacy (other timeframes):** default, fast, 1h-light, 4h-balanced\n",
    "- Ho·∫∑c c·∫•u h√¨nh th·ªß c√¥ng b·∫±ng c√°ch set PRESET_NAME = None\n",
    "- Th√™m c√°c features kh√°c (volume, open, high, low)\n",
    "- Th·ª≠ timeframe kh√°c (1d, 4h, 1h, 15m) - default l√† 15m\n",
    "\n",
    "**Ch√∫c b·∫°n th√†nh c√¥ng! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
