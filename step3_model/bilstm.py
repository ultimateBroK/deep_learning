"""
B∆Ø·ªöC 3: X√ÇY D·ª∞NG MODEL BiLSTM
-------------------------------

Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
- LSTM (Long Short-Term Memory) gi·ªëng nh∆∞ b·ªô nh·ªõ ng·∫Øn h·∫°n
- BiLSTM (Bidirectional LSTM) nh√¨n c·∫£ qu√° kh·ª© V√Ä t∆∞∆°ng lai
- T·∫°i sao nh√¨n t∆∞∆°ng lai ƒë∆∞·ª£c? V√¨ khi train, ta c√≥ to√†n b·ªô d·ªØ li·ªáu!

V√≠ d·ª•:
- LSTM th∆∞·ªùng: H√¥m nay d·ª±a tr√™n 60 ng√†y tr∆∞·ªõc
- BiLSTM: H√¥m nay d·ª±a tr√™n 60 ng√†y tr∆∞·ªõc + 60 ng√†y sau (khi train)

L·ª£i √≠ch:
- Ph√°t hi·ªán pattern t·ªët h∆°n
- Hi·ªÉu context t·ª´ 2 ph√≠a
- K·∫øt qu·∫£ th∆∞·ªùng t·ªët h∆°n LSTM th∆∞·ªùng
"""

from typing import Tuple
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models


def build_bilstm_model(
    input_shape: Tuple[int, int],
    lstm_units: list = [64, 32],
    dropout_rate: float = 0.2,
    dense_units: list = [16],
    output_units: int = 1
) -> models.Sequential:
    """
    X√¢y d·ª±ng model BiLSTM
    
    Args:
        input_shape: Shape c·ªßa d·ªØ li·ªáu ƒë·∫ßu v√†o (window_size, n_features)
        lstm_units: List s·ªë units cho m·ªói LSTM layer
        dropout_rate: T·ª∑ l·ªá dropout (ƒë·ªÉ tr√°nh overfitting)
        dense_units: List s·ªë units cho m·ªói Dense layer
        output_units: S·ªë units ·ªü output layer (th∆∞·ªùng = 1 cho d·ª± ƒëo√°n gi√°)
    
    Returns:
        Model BiLSTM ƒë√£ ƒë∆∞·ª£c compile
    
    V√≠ d·ª•:
        input_shape = (60, 1)  # 60 ng√†y, 1 feature (gi√° close)
        lstm_units = [64, 32]   # 2 LSTM layers v·ªõi 64 v√† 32 units
        output_units = 1       # D·ª± ƒëo√°n 1 gi√° tr·ªã (gi√° ng√†y mai)
    """
    model = models.Sequential(name="BiLSTM_Price_Prediction")
    
    # Layer ƒë·∫ßu ti√™n: Input layer (ƒë·ªÉ tr√°nh warning v·ªÅ input_shape)
    model.add(layers.Input(shape=input_shape))
    
    # Layer th·ª© hai: Bidirectional LSTM
    # return_sequences=True ƒë·ªÉ pass cho LSTM layer ti·∫øp theo
    model.add(layers.Bidirectional(
        layers.LSTM(
            lstm_units[0],
            return_sequences=len(lstm_units) > 1,
            name="bilstm_1"
        ),
        name="bidirectional_1"
    ))
    model.add(layers.Dropout(dropout_rate, name="dropout_1"))
    
    # C√°c LSTM layers ti·∫øp theo (n·∫øu c√≥)
    for i, units in enumerate(lstm_units[1:], start=2):
        model.add(layers.Bidirectional(
            layers.LSTM(
                units,
                return_sequences=i < len(lstm_units),
                name=f"bilstm_{i}"
            ),
            name=f"bidirectional_{i}"
        ))
        model.add(layers.Dropout(dropout_rate, name=f"dropout_{i}"))
    
    # Dense layers
    for i, units in enumerate(dense_units, start=1):
        model.add(layers.Dense(units, activation='relu', name=f"dense_{i}"))
        model.add(layers.Dropout(dropout_rate * 0.5, name=f"dense_dropout_{i}"))
    
    # Output layer
    model.add(layers.Dense(output_units, name="output"))
    
    # Compile model
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',  # Mean Squared Error - t·ªët cho regression
        metrics=['mae']  # Mean Absolute Error - d·ªÖ hi·ªÉu h∆°n
    )
    
    print(f"‚úÖ ƒê√£ build model BiLSTM v·ªõi {len(lstm_units)} LSTM layers")
    
    return model


def print_model_summary(model: models.Sequential):
    """
    In th√¥ng tin chi ti·∫øt v·ªÅ model
    """
    print("\n" + "="*60)
    print("MODEL SUMMARY")
    print("="*60)
    model.summary()
    print("="*60)
    
    # ƒê·∫øm s·ªë l∆∞·ª£ng parameters
    total_params = model.count_params()
    trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])
    non_trainable_params = total_params - trainable_params
    
    print("\nüìä Th·ªëng k√™:")
    print(f"   Total parameters: {total_params:,}")
    print(f"   Trainable: {trainable_params:,}")
    print(f"   Non-trainable: {non_trainable_params:,}")


if __name__ == "__main__":
    # Test function
    input_shape = (60, 1)  # 60 ng√†y, 1 feature
    model = build_bilstm_model(input_shape=input_shape)
    print_model_summary(model)
