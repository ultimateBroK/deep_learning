"""
BÆ¯á»šC 4.1: TRAINING MODEL
--------------------------

Giáº£i thÃ­ch báº±ng vÃ­ dá»¥ Ä‘á»i sá»‘ng:
- Training giá»‘ng nhÆ° báº¡n táº­p lÃ m bÃ i táº­p
- Model há»c tá»« cÃ¡c vÃ­ dá»¥ (data) Ä‘á»ƒ tÃ¬m pattern
- Má»—i epoch = 1 láº§n há»c háº¿t toÃ n bá»™ bÃ i táº­p

VÃ­ dá»¥:
- Epoch 1: Model há»c tá»« data láº§n Ä‘áº§u tiÃªn, chÆ°a hiá»ƒu nhiá»u
- Epoch 2: Model há»c láº¡i, hiá»ƒu rÃµ hÆ¡n
- Epoch 20: Model Ä‘Ã£ hiá»ƒu tá»‘t pattern cá»§a dá»¯ liá»‡u

Callback lÃ  gÃ¬?
- Giá»‘ng nhÆ° "ngÆ°á»i giÃ¡m sÃ¡t" trong quÃ¡ trÃ¬nh training
- EarlyStopping: Dá»«ng láº¡i khi model khÃ´ng cÃ²n há»c Ä‘Æ°á»£c gÃ¬
- ModelCheckpoint: LÆ°u láº¡i model tá»‘t nháº¥t
- ReduceLROnPlateau: Giáº£m learning rate khi model khÃ´ng cÃ²n tiáº¿n bá»™
"""

from pathlib import Path
import time
import numpy as np
from typing import Dict
from tensorflow import keras
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau


def train_model(
    model: keras.Model,
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_val: np.ndarray,
    y_val: np.ndarray,
    epochs: int = 20,
    batch_size: int = 32,
    checkpoint_dir: str = None,
    checkpoint_name: str = "best_model.keras",
    early_stopping_patience: int = 5
) -> Dict:
    """
    Huáº¥n luyá»‡n model
    
    Args:
        model: Model Ä‘Ã£ Ä‘Æ°á»£c build
        X_train, y_train: Dá»¯ liá»‡u train
        X_val, y_val: Dá»¯ liá»‡u validation
        epochs: Sá»‘ láº§n há»c qua toÃ n bá»™ data
        batch_size: Sá»‘ samples má»—i láº§n tÃ­nh gradient
        checkpoint_dir: ThÆ° má»¥c lÆ°u model
        checkpoint_name: TÃªn file checkpoint
        early_stopping_patience: Sá»‘ epochs chá» trÆ°á»›c khi dá»«ng
    
    Returns:
        Dictionary chá»©a:
            - history: Training history
            - best_epoch: Epoch cÃ³ val_loss tháº¥p nháº¥t
            - callbacks: List callbacks Ä‘Ã£ dÃ¹ng
    """
    # Táº¡o thÆ° má»¥c checkpoint
    if checkpoint_dir is None:
        checkpoint_dir = Path(__file__).parent.parent / "reports" / "checkpoints"
    else:
        checkpoint_dir = Path(checkpoint_dir)
    
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    checkpoint_path = checkpoint_dir / checkpoint_name
    
    # 1. ModelCheckpoint: LÆ°u láº¡i model tá»‘t nháº¥t
    checkpoint_callback = ModelCheckpoint(
        filepath=str(checkpoint_path),
        monitor='val_loss',
        save_best_only=True,
        verbose=1,
        mode='min'
    )
    
    # 2. EarlyStopping: Dá»«ng láº¡i náº¿u val_loss khÃ´ng giáº£m
    early_stop_callback = EarlyStopping(
        monitor='val_loss',
        patience=early_stopping_patience,
        restore_best_weights=True,
        verbose=1,
        mode='min'
    )
    
    # 3. ReduceLROnPlateau: Giáº£m learning rate náº¿u val_loss khÃ´ng giáº£m
    reduce_lr_callback = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,      # Giáº£m LR Ä‘i 50%
        patience=3,       # Chá» 3 epochs
        min_lr=1e-6,      # LR tá»‘i thiá»ƒu
        verbose=1
    )
    
    callbacks = [checkpoint_callback, early_stop_callback, reduce_lr_callback]
    
    print("\n" + "=" * 60)
    print("ğŸš€ Báº®T Äáº¦U TRAINING")
    print("=" * 60)
    print(f"Epochs: {epochs}")
    print(f"Batch size: {batch_size}")
    print(f"Train samples: {len(X_train)}")
    print(f"Val samples: {len(X_val)}")
    print(f"Checkpoint: {checkpoint_path}")
    print("=" * 60 + "\n")
    
    # Training
    t0 = time.perf_counter()
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1
    )
    train_seconds = time.perf_counter() - t0
    
    # TÃ¬m epoch cÃ³ val_loss tháº¥p nháº¥t
    best_epoch = np.argmin(history.history['val_loss']) + 1  # +1 vÃ¬ epoch báº¯t Ä‘áº§u tá»« 1
    best_val_loss = min(history.history['val_loss'])
    
    print("\n" + "=" * 60)
    print("âœ… TRAINING HOÃ€N THÃ€NH")
    print("=" * 60)
    print(f"Best epoch: {best_epoch}/{epochs}")
    print(f"Best val_loss: {best_val_loss:.6f}")
    print(f"Best val_mae: {history.history['val_mae'][best_epoch-1]:.6f}")
    print(f"Training time: {train_seconds:.2f}s")
    print("=" * 60 + "\n")
    
    return {
        "history": history,
        "best_epoch": best_epoch,
        "best_val_loss": best_val_loss,
        "train_seconds": train_seconds,
        "callbacks": callbacks,
        "checkpoint_path": checkpoint_path
    }


def load_checkpoint(checkpoint_path: str) -> keras.Model:
    """
    Load model tá»« checkpoint
    
    Args:
        checkpoint_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file .keras hoáº·c .h5
    
    Returns:
        Model Ä‘Ã£ load
    """
    model = keras.models.load_model(checkpoint_path)
    print(f"âœ… ÄÃ£ load model tá»«: {checkpoint_path}")
    return model


def clean_checkpoints(checkpoint_dir: str = None, keep_best: bool = True) -> int:
    """
    XÃ³a cÃ¡c checkpoint cÅ©
    
    Args:
        checkpoint_dir: ThÆ° má»¥c checkpoint
        keep_best: CÃ³ giá»¯ láº¡i checkpoint "best" khÃ´ng
    
    Returns:
        Sá»‘ file Ä‘Ã£ xÃ³a
    """
    if checkpoint_dir is None:
        checkpoint_dir = Path(__file__).parent.parent / "reports" / "checkpoints"
    else:
        checkpoint_dir = Path(checkpoint_dir)
    
    if not checkpoint_dir.exists():
        return 0
    
    deleted_count = 0
    
    for file_path in checkpoint_dir.glob("*.keras"):
        if keep_best and "best" in file_path.name.lower():
            continue
        
        file_path.unlink()
        deleted_count += 1
    
    if deleted_count > 0:
        print(f"ğŸ—‘ï¸  ÄÃ£ xÃ³a {deleted_count} checkpoint")
    else:
        print("âœ… KhÃ´ng cÃ³ checkpoint nÃ o Ä‘á»ƒ xÃ³a")
    
    return deleted_count


if __name__ == "__main__":
    # Test function (cáº§n cÃ³ model vÃ  data)
    print("Testing train_model...")
    print("Cáº§n táº¡o model vÃ  data trÆ°á»›c khi cháº¡y!")
