#!/usr/bin/env python3
"""
üéØ ENTRY POINT: CHAY PROJECT CLI
---------------------------------

Gi·∫£i th√≠ch:
- File n√†y l√† "c·ª≠a ch√≠nh" ƒë·ªÉ ch·∫°y to√†n b·ªô project
- Ch·∫°y t·ª´ terminal v·ªõi c√°c tham s·ªë
- T·ª± ƒë·ªông ch·∫°y qua t·∫•t c·∫£ c√°c b∆∞·ªõc

C√°ch d√πng:
    python main.py --epochs 20 --limit 1500
"""

import argparse
import sys
from pathlib import Path
import re

# Th√™m th∆∞ m·ª•c g·ªëc v√†o path
sys.path.insert(0, str(Path(__file__).parent))

# L∆∞u √Ω: kh√¥ng import c√°c module "n·∫∑ng" ·ªü top-level ƒë·ªÉ `python main.py --help` ch·∫°y g·ªçn v√† nhanh.


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="D·ª± b√°o gi√° Bitcoin v·ªõi BiLSTM",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª•:
  python main.py --epochs 20 --limit 1500
  python main.py --timeframe 4h --window 30
  python main.py --refresh-cache
        """
    )
    
    # Data args
    parser.add_argument(
        '--data-path',
        type=str,
        default=None,
        help='ƒê∆∞·ªùng d·∫´n file CSV (n·∫øu b·ªè tr·ªëng s·∫Ω ch·ªçn theo --timeframe trong th∆∞ m·ª•c data/)'
    )
    parser.add_argument(
        '--timeframe',
        type=str,
        default='1d',
        choices=['1d', '4h'],
        help='Timeframe (d√πng ƒë·ªÉ ch·ªçn file m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng set --data-path) (m·∫∑c ƒë·ªãnh: 1d)'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=1500,
        help='L·∫•y N d√≤ng cu·ªëi trong file CSV (m·∫∑c ƒë·ªãnh: 1500, <=0 = l·∫•y t·∫•t c·∫£)'
    )
    parser.add_argument(
        '--refresh-cache',
        action='store_true',
        help='ƒê·ªçc l·∫°i t·ª´ CSV g·ªëc (b·ªè qua cache ƒë√£ chu·∫©n ho√°)'
    )
    
    # Preprocessing args
    parser.add_argument(
        '--window',
        type=int,
        default=60,
        help='S·ªë n·∫øn nh√¨n l·∫°i (m·∫∑c ƒë·ªãnh: 60)'
    )
    parser.add_argument(
        '--features',
        type=str,
        nargs='+',
        default=['close'],
        help='Features s·ª≠ d·ª•ng (m·∫∑c ƒë·ªãnh: close)'
    )
    
    # Model args
    parser.add_argument(
        '--lstm-units',
        type=int,
        nargs='+',
        default=[64, 32],
        help='S·ªë units cho m·ªói LSTM layer (m·∫∑c ƒë·ªãnh: 64 32)'
    )
    parser.add_argument(
        '--dropout',
        type=float,
        default=0.2,
        help='Dropout rate (m·∫∑c ƒë·ªãnh: 0.2)'
    )
    
    # Training args
    parser.add_argument(
        '--epochs',
        type=int,
        default=20,
        help='S·ªë epochs (m·∫∑c ƒë·ªãnh: 20)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=32,
        help='Batch size (m·∫∑c ƒë·ªãnh: 32)'
    )
    
    # Runtime args
    parser.add_argument(
        '--intra-threads',
        type=int,
        default=12,
        help='CPU threads cho operations (m·∫∑c ƒë·ªãnh: 12)'
    )

    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='C·ªë ƒë·ªãnh ng·∫´u nhi√™n ƒë·ªÉ t√°i l·∫≠p k·∫øt qu·∫£ (m·∫∑c ƒë·ªãnh: 42, <0 = kh√¥ng set)'
    )
    
    return parser.parse_args()


def _infer_timeframe_from_filename(path_str: str | None) -> str | None:
    """
    Infer timeframe d·ª±a v√†o t√™n file, v√≠ d·ª•:
    - btc_1d_data_2018_to_2025.csv -> 1d
    - btc_4h_data_2018_to_2025.csv -> 4h
    """
    if not path_str:
        return None
    name = Path(path_str).name.lower()
    if re.search(r"(?:^|_)4h(?:_|\\.)", name) or "4h" in name:
        return "4h"
    if re.search(r"(?:^|_)1d(?:_|\\.)", name) or "1d" in name:
        return "1d"
    return None


def _default_data_path_from_timeframe(timeframe: str) -> str:
    tf = (timeframe or "1d").lower()
    base = Path(__file__).parent / "data"
    if tf == "4h":
        return str(base / "btc_4h_data_2018_to_2025.csv")
    return str(base / "btc_1d_data_2018_to_2025.csv")


def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y project"""
    # Parse args
    args = parse_args()

    # Import c√°c module "n·∫∑ng" sau khi parse args ƒë·ªÉ:
    # - `python main.py --help` ch·∫°y nhanh v√† kh√¥ng in log TensorFlow
    from step1_data import fetch_binance_data
    from step2_preprocessing import prepare_data_for_lstm
    from step3_model import build_bilstm_model, print_model_summary
    from step4_training import (
        train_model,
        evaluate_model,
        print_sample_predictions,
        calculate_direction_accuracy,
    )
    from step5_visualization import plot_training_history, plot_predictions, plot_all_in_one
    from utils import (
        configure_tensorflow_runtime,
        print_tensorflow_info,
        create_results_folder,
        save_markdown_report,
        save_config,
        save_metrics,
        set_random_seed,
    )
    
    print("\n" + "="*70)
    print(" " * 15 + "D·ª∞ B√ÅO GI√Å BITCOIN V·ªöI BiLSTM")
    print("="*70)
    
    # C·∫•u h√¨nh TensorFlow
    set_random_seed(args.seed, deterministic=True)
    configure_tensorflow_runtime(
        intra_op_threads=args.intra_threads,
        inter_op_threads=2,
        enable_xla=True
    )
    print_tensorflow_info()
    
    # ========================================
    # B∆Ø·ªöC 1: L·∫§Y D·ªÆ LI·ªÜU
    # ========================================
    print("\n" + "="*70)
    print("B∆Ø·ªöC 1: ƒê·ªåC D·ªÆ LI·ªÜU CSV (LOCAL)")
    print("="*70 + "\n")

    data_path = args.data_path or _default_data_path_from_timeframe(args.timeframe)
    inferred_tf = _infer_timeframe_from_filename(data_path)
    effective_tf = inferred_tf or args.timeframe
    print(f"üìÑ Data file: {data_path}")
    print(f"üïí Timeframe (t·ª´ t√™n file): {effective_tf}\n")
    
    df = fetch_binance_data(
        data_path=data_path,
        timeframe=effective_tf,
        limit=args.limit,
        save_cache=not args.refresh_cache
    )

    # Th√¥ng tin d·ªØ li·ªáu (ƒë∆∞a v√†o report)
    data_rows = len(df)
    try:
        data_start = str(df["datetime"].iloc[0])
        data_end = str(df["datetime"].iloc[-1])
    except Exception:
        data_start, data_end = None, None
    
    # ========================================
    # B∆Ø·ªöC 2: X·ª¨ L√ù D·ªÆ LI·ªÜU
    # ========================================
    print("\n" + "="*70)
    print("B∆Ø·ªöC 2: X·ª¨ L√ù D·ªÆ LI·ªÜU")
    print("="*70 + "\n")
    
    data_dict = prepare_data_for_lstm(
        df=df,
        features=args.features,
        window_size=args.window,
        scaler_type='minmax'
    )
    
    X_train = data_dict['X_train']
    y_train = data_dict['y_train']
    X_val = data_dict['X_val']
    y_val = data_dict['y_val']
    X_test = data_dict['X_test']
    y_test = data_dict['y_test']
    scaler = data_dict['scaler']

    # Th√¥ng tin split (ƒë∆∞a v√†o report)
    train_samples = len(X_train)
    val_samples = len(X_val)
    test_samples = len(X_test)
    scaler_type = "minmax"
    
    # ========================================
    # B∆Ø·ªöC 3: X√ÇY D·ª∞NG MODEL
    # ========================================
    print("\n" + "="*70)
    print("B∆Ø·ªöC 3: X√ÇY D·ª∞NG MODEL BiLSTM")
    print("="*70 + "\n")
    
    input_shape = (args.window, len(args.features))
    model = build_bilstm_model(
        input_shape=input_shape,
        lstm_units=args.lstm_units,
        dropout_rate=args.dropout,
        dense_units=[16],
        output_units=1
    )
    print_model_summary(model)
    
    # ========================================
    # B∆Ø·ªöC 4: TRAINING
    # ========================================
    print("\n" + "="*70)
    print("B∆Ø·ªöC 4: TRAINING MODEL")
    print("="*70 + "\n")
    
    train_result = train_model(
        model=model,
        X_train=X_train,
        y_train=y_train,
        X_val=X_val,
        y_val=y_val,
        epochs=args.epochs,
        batch_size=args.batch_size,
        early_stopping_patience=5
    )
    
    history = train_result['history']
    best_epoch = train_result.get("best_epoch")
    best_val_loss = train_result.get("best_val_loss")
    train_seconds = train_result.get("train_seconds")
    checkpoint_path = str(train_result.get("checkpoint_path")) if train_result.get("checkpoint_path") is not None else None
    
    # ========================================
    # B∆Ø·ªöC 5: ƒê√ÅNH GI√Å & V·∫º BI·ªÇU ƒê·ªí
    # ========================================
    print("\n" + "="*70)
    print("B∆Ø·ªöC 5: ƒê√ÅNH GI√Å & V·∫º BI·ªÇU ƒê·ªí")
    print("="*70 + "\n")
    
    # ƒê√°nh gi√° tr√™n test set
    eval_result = evaluate_model(
        model=model,
        X_test=X_test,
        y_test=y_test,
        scaler=scaler,
        return_predictions=True
    )
    
    y_true = eval_result['y_true']
    y_pred = eval_result['predictions']
    
    # In m·ªôt s·ªë v√≠ d·ª• d·ª± ƒëo√°n
    print_sample_predictions(y_true, y_pred, n_samples=10)
    
    # T√≠nh ƒë·ªô ch√≠nh x√°c xu h∆∞·ªõng
    direction_accuracy = calculate_direction_accuracy(y_true, y_pred)
    eval_result["direction_accuracy"] = float(direction_accuracy)
    
    # ========================================
    # L∆ØU K·∫æT QU·∫¢
    # ========================================
    print("\n" + "="*70)
    print("L∆ØU K·∫æT QU·∫¢")
    print("="*70 + "\n")
    
    # T·∫°o folder k·∫øt qu·∫£
    results_folder = create_results_folder(run_type="main")
    print(f"\nüìÅ Folder k·∫øt qu·∫£: {results_folder}\n")
    
    # V·∫Ω v√† l∆∞u bi·ªÉu ƒë·ªì
    timestamp_suffix = results_folder.name.replace('BiLSTM_', '')
    
    plot_history_file = results_folder / f"training_history_{timestamp_suffix}.png"
    plot_predictions_file = results_folder / f"predictions_{timestamp_suffix}.png"
    plot_all_in_one_file = results_folder / f"all_in_one_{timestamp_suffix}.png"
    
    plot_training_history(history, save_path=str(plot_history_file))
    plot_predictions(y_true, y_pred, save_path=str(plot_predictions_file))
    plot_all_in_one(history, y_true, y_pred, save_path=str(plot_all_in_one_file))
    
    # L∆∞u b√°o c√°o
    config_dict = {
        'data_path': data_path,
        'timeframe': effective_tf,
        'limit': args.limit,
        'data_rows': data_rows,
        'data_start': data_start,
        'data_end': data_end,
        'window_size': args.window,
        'features': args.features,
        'scaler_type': scaler_type,
        'train_samples': train_samples,
        'val_samples': val_samples,
        'test_samples': test_samples,
        'lstm_units': args.lstm_units,
        'dropout_rate': args.dropout,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'intra_threads': args.intra_threads,
        'seed': args.seed,
        'best_epoch': best_epoch,
        'best_val_loss': best_val_loss,
        'train_seconds': train_seconds,
        'checkpoint_path': checkpoint_path,
    }
    
    plots_dict = {
        'training_history': timestamp_suffix,
        'predictions': timestamp_suffix,
        'all_in_one': timestamp_suffix
    }
    
    save_markdown_report(
        folder_path=results_folder,
        config=config_dict,
        metrics=eval_result,
        history=history.history,
        plots=plots_dict
    )
    save_config(results_folder, config_dict)
    save_metrics(results_folder, eval_result)
    
    # ========================================
    # HO√ÄN TH√ÄNH
    # ========================================
    print("\n" + "="*70)
    print("‚úÖ HO√ÄN TH√ÄNH!")
    print("="*70)
    print(f"üìä B√°o c√°o: {results_folder / f'results_BiLSTM_{timestamp_suffix}.md'}")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
