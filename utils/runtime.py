"""
UTILS: C·∫§U H√åNH RUNTIME TENSORFLOW
------------------------------------

Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
- TensorFlow c√≥ nhi·ªÅu c√°ch ch·∫°y (CPU, GPU, TPU)
- V·ªõi CPU AMD, c·∫ßn c·∫•u h√¨nh s·ªë threads ƒë·ªÉ t·ªëi ∆∞u
- Gi·ªëng nh∆∞ b·∫°n c√≥ 12 nh√¢n CPU, n√™n t·∫≠n d·ª•ng h·∫øt

L∆∞u √Ω:
- intra_op_parallelism_threads: S·ªë thread cho c√°c operations song song
- inter_op_parallelism_threads: S·ªë thread cho c√°c operations song song kh√°c
- enable_xla: T·ªëi ∆∞u code b·∫±ng XLA (Accelerated Linear Algebra)
"""

import os
import tensorflow as tf


def configure_tensorflow_runtime(
    intra_op_threads: int = 12,
    inter_op_threads: int = 2,
    enable_xla: bool = True
):
    """
    C·∫•u h√¨nh runtime TensorFlow cho CPU AMD
    
    Args:
        intra_op_threads: S·ªë thread cho operations song song (s·ªë core v·∫≠t l√Ω)
        inter_op_threads: S·ªë thread cho operations song song kh√°c
        enable_xla: B·∫≠t XLA optimization
    """
    # C·∫•u h√¨nh s·ªë threads
    tf.config.threading.set_intra_op_parallelism_threads(intra_op_threads)
    tf.config.threading.set_inter_op_parallelism_threads(inter_op_threads)
    
    # B·∫≠t XLA optimization
    if enable_xla:
        os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'
    
    # C·∫•u h√¨nh CPU affinity (ch·ªâ ch·∫°y tr√™n CPU)
    tf.config.set_visible_devices([], 'GPU')
    
    # In th√¥ng tin c·∫•u h√¨nh
    print(f"{'='*60}")
    print(f"‚öôÔ∏è  C·∫§U H√åNH TENSORFLOW RUNTIME")
    print(f"{'='*60}")
    print(f"Intra-op threads: {intra_op_threads}")
    print(f"Inter-op threads: {inter_op_threads}")
    print(f"XLA enabled: {enable_xla}")
    print(f"CPU only: True")
    print(f"{'='*60}\n")


def get_gpu_info():
    """
    Ki·ªÉm tra GPU c√≥ s·∫µn kh√¥ng
    
    Returns:
        True n·∫øu c√≥ GPU, False n·∫øu kh√¥ng
    """
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        print(f"‚úÖ T√¨m th·∫•y {len(gpus)} GPU:")
        for gpu in gpus:
            print(f"   - {gpu.name}")
        return True
    else:
        print("‚ÑπÔ∏è  Kh√¥ng t√¨m th·∫•y GPU, s·∫Ω d√πng CPU")
        return False


def set_memory_growth():
    """
    Cho ph√©p GPU t·ª± tƒÉng b·ªô nh·ªõ khi c·∫ßn (tr√°nh chi·∫øm h·∫øt VRAM)
    """
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print("‚úÖ ƒê√£ b·∫≠t memory growth cho GPU")
        except RuntimeError as e:
            print(f"‚ùå L·ªói khi c·∫•u h√¨nh GPU: {e}")


def print_tensorflow_info():
    """
    In th√¥ng tin v·ªÅ TensorFlow v√† runtime
    """
    print(f"\n{'='*60}")
    print(f"üìã TH√îNG TIN TENSORFLOW")
    print(f"{'='*60}")
    print(f"TensorFlow version: {tf.__version__}")
    print(f"Keras version: {tf.keras.__version__}")
    print(f"Built with CUDA: {tf.test.is_built_with_cuda()}")
    print(f"GPU available: {get_gpu_info()}")
    
    # CPU threads
    print(f"Intra-op threads: {tf.config.threading.get_intra_op_parallelism_threads()}")
    print(f"Inter-op threads: {tf.config.threading.get_inter_op_parallelism_threads()}")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    # Test functions
    configure_tensorflow_runtime()
    print_tensorflow_info()
