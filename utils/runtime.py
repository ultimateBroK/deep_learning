"""
UTILS: Cáº¤U HÃŒNH RUNTIME TENSORFLOW
------------------------------------

Giáº£i thÃ­ch báº±ng vÃ­ dá»¥ Ä‘á»i sá»‘ng:
- TensorFlow cÃ³ nhiá»u cÃ¡ch cháº¡y (CPU, GPU, TPU)
- Vá»›i CPU AMD, cáº§n cáº¥u hÃ¬nh sá»‘ threads Ä‘á»ƒ tá»‘i Æ°u
- Giá»‘ng nhÆ° báº¡n cÃ³ 12 nhÃ¢n CPU, nÃªn táº­n dá»¥ng háº¿t

LÆ°u Ã½:
- intra_op_parallelism_threads: Sá»‘ thread cho cÃ¡c operations song song
- inter_op_parallelism_threads: Sá»‘ thread cho cÃ¡c operations song song khÃ¡c
- enable_xla: Tá»‘i Æ°u code báº±ng XLA (Accelerated Linear Algebra)
"""

import os
import random
import warnings

import numpy as np

# Suppress warnings trÆ°á»›c khi import TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Chá»‰ hiá»ƒn thá»‹ ERROR vÃ  WARNING nghiÃªm trá»ng
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Táº¯t oneDNN warnings

# Suppress FutureWarning vá» np.object
warnings.filterwarnings('ignore', category=FutureWarning, message='.*np.object.*')
warnings.filterwarnings('ignore', message='.*oneDNN.*')
warnings.filterwarnings('ignore', message='.*CUDA.*')

import tensorflow as tf

# Suppress TensorFlow warnings sau khi import
tf.get_logger().setLevel('ERROR')


def set_random_seed(seed: int, deterministic: bool = False) -> None:
    """
    Cá»‘ Ä‘á»‹nh ngáº«u nhiÃªn Ä‘á»ƒ káº¿t quáº£ cháº¡y cÃ³ thá»ƒ tÃ¡i láº­p.

    Args:
        seed: Sá»‘ seed (vÃ­ dá»¥ 42). Náº¿u seed < 0 thÃ¬ khÃ´ng lÃ m gÃ¬.
        deterministic: Cá»‘ gáº¯ng báº­t deterministic ops (best-effort, tuá»³ mÃ´i trÆ°á»ng/TF version).
    """
    if seed is None or seed < 0:
        return

    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)

    try:
        tf.keras.utils.set_random_seed(seed)
    except Exception:
        tf.random.set_seed(seed)

    if deterministic:
        os.environ.setdefault("TF_DETERMINISTIC_OPS", "1")
        try:
            tf.config.experimental.enable_op_determinism()
        except Exception:
            pass


def configure_tensorflow_runtime(
    intra_op_threads: int = 12,
    inter_op_threads: int = 2,
    enable_xla: bool = True
):
    """
    Cáº¥u hÃ¬nh runtime TensorFlow cho CPU AMD
    
    Args:
        intra_op_threads: Sá»‘ thread cho operations song song (sá»‘ core váº­t lÃ½)
        inter_op_threads: Sá»‘ thread cho operations song song khÃ¡c
        enable_xla: Báº­t XLA optimization
    """
    # Suppress warnings khi cáº¥u hÃ¬nh
    old_level = tf.get_logger().level
    tf.get_logger().setLevel('ERROR')
    
    try:
        # Cáº¥u hÃ¬nh sá»‘ threads
        tf.config.threading.set_intra_op_parallelism_threads(intra_op_threads)
        tf.config.threading.set_inter_op_parallelism_threads(inter_op_threads)
        
        # Báº­t XLA optimization
        if enable_xla:
            os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'
        
        # Chá»‰ cháº¡y trÃªn CPU (náº¿u cÃ³ GPU thÃ¬ áº©n Ä‘i; náº¿u khÃ´ng cÃ³ GPU thÃ¬ bá» qua)
        try:
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                tf.config.set_visible_devices([], 'GPU')
        except Exception as e:
            # KhÃ´ng Ä‘á»ƒ lá»—i runtime nhá» lÃ m há»ng toÃ n pipeline
            pass  # Suppress error message
    finally:
        tf.get_logger().setLevel(old_level)
    
    # In thÃ´ng tin cáº¥u hÃ¬nh
    print("=" * 60)
    print("âš™ï¸  Cáº¤U HÃŒNH TENSORFLOW RUNTIME")
    print("=" * 60)
    print(f"Intra-op threads: {intra_op_threads}")
    print(f"Inter-op threads: {inter_op_threads}")
    print(f"XLA enabled: {enable_xla}")
    print("CPU only: True")
    print("=" * 60 + "\n")


def get_gpu_info():
    """
    Kiá»ƒm tra GPU cÃ³ sáºµn khÃ´ng
    
    Returns:
        True náº¿u cÃ³ GPU, False náº¿u khÃ´ng
    """
    # Suppress warnings khi check GPU
    old_level = tf.get_logger().level
    tf.get_logger().setLevel('ERROR')
    
    try:
        gpus = tf.config.list_physical_devices('GPU')
    finally:
        tf.get_logger().setLevel(old_level)
    
    if gpus:
        print(f"âœ… TÃ¬m tháº¥y {len(gpus)} GPU:")
        for gpu in gpus:
            print(f"   - {gpu.name}")
        return True
    else:
        print("â„¹ï¸  KhÃ´ng tÃ¬m tháº¥y GPU, sáº½ dÃ¹ng CPU")
        return False


def set_memory_growth():
    """
    Cho phÃ©p GPU tá»± tÄƒng bá»™ nhá»› khi cáº§n (trÃ¡nh chiáº¿m háº¿t VRAM)
    """
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print("âœ… ÄÃ£ báº­t memory growth cho GPU")
        except RuntimeError as e:
            print(f"âŒ Lá»—i khi cáº¥u hÃ¬nh GPU: {e}")


def print_tensorflow_info():
    """
    In thÃ´ng tin vá» TensorFlow vÃ  runtime
    """
    print("\n" + "=" * 60)
    print("ðŸ“‹ THÃ”NG TIN TENSORFLOW")
    print("=" * 60)
    print(f"TensorFlow version: {tf.__version__}")
    print(f"Keras version: {tf.keras.__version__}")
    print(f"Built with CUDA: {tf.test.is_built_with_cuda()}")
    print(f"GPU available: {get_gpu_info()}")
    
    # CPU threads
    print(f"Intra-op threads: {tf.config.threading.get_intra_op_parallelism_threads()}")
    print(f"Inter-op threads: {tf.config.threading.get_inter_op_parallelism_threads()}")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    # Test functions
    configure_tensorflow_runtime()
    print_tensorflow_info()
