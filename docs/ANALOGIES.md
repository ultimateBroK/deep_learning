# ğŸ“ Analogies - Giáº£i ThÃ­ch Báº±ng VÃ­ Dá»¥ Äá»i Sá»‘ng

Äá»c cÃ¡c khÃ¡i niá»‡m ML/DL báº±ng vÃ­ dá»¥ Ä‘á»i sá»‘ng, dá»… hiá»ƒu hÆ¡n bao giá» háº¿t!

---

## ğŸ“‹ Chá»‰ Má»¥c

- [BiLSTM](#bilstm-bidirectional-lstm)
- [Sliding Window](#sliding-window)
- [Scaling](#scaling-chuáº©n-hÃ³a-dá»¯-liá»‡u)
- [Train/Validation/Test Split](#trainvalidationtest-split)
- [Epochs](#epochs)
- [Batch Size](#batch-size)
- [Learning Rate](#learning-rate)
- [Dropout](#dropout)
- [Overfitting vs Underfitting](#overfitting-vs-underfitting)
- [Loss Function](#loss-function)
- [Callbacks](#callbacks)

---

## BiLSTM (Bidirectional LSTM)

### Giáº£i thÃ­ch ML
- LSTM: Neural network cÃ³ kháº£ nÄƒng ghi nhá»› thÃ´ng tin dÃ i háº¡n
- BiLSTM: LSTM nhÃ¬n cáº£ quÃ¡ khá»© VÃ€ tÆ°Æ¡ng lai

### VÃ­ dá»¥ Ä‘á»i sá»‘ng: Äá»c má»™t cÃ¢u vÄƒn

**LSTM thÆ°á»ng (uni-directional):**
- Báº¡n Ä‘á»c cÃ¢u tá»« trÃ¡i â†’ pháº£i
- Khi Ä‘á»c tá»« cuá»‘i, báº¡n Ä‘Ã£ quÃªn tá»« Ä‘áº§u
- VÃ­ dá»¥: "HÃ´m nay trá»i ráº¥t Ä‘áº¹p, tÃ´i thÃ­ch Ä‘i..."
- Khi Ä‘á»c Ä‘áº¿n "Ä‘i", báº¡n nhá»› "thÃ­ch" nhÆ°ng Ä‘Ã£ quÃªn "HÃ´m nay"

**BiLSTM (bi-directional):**
- Báº¡n Ä‘á»c cÃ¢u 2 láº§n: trÃ¡i â†’ pháº£i VÃ€ pháº£i â†’ trÃ¡i
- Khi Ä‘á»c báº¥t ká»³ tá»« nÃ o, báº¡n Ä‘á»u biáº¿t cáº£ pháº§n trÆ°á»›c vÃ  sau
- VÃ­ dá»¥: "HÃ´m nay trá»i ráº¥t Ä‘áº¹p, tÃ´i thÃ­ch Ä‘i dáº¡o."
- Khi Ä‘á»c "thÃ­ch", báº¡n biáº¿t: trÆ°á»›c lÃ  "trá»i ráº¥t Ä‘áº¹p", sau lÃ  "Ä‘i dáº¡o"

### Táº¡i sao BiLSTM tá»‘t hÆ¡n cho dá»± Ä‘oÃ¡n giÃ¡?
- GiÃ¡ Bitcoin bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi cáº£ quÃ¡ khá»© (60 ngÃ y trÆ°á»›c) VÃ€ tÆ°Æ¡ng lai (60 ngÃ y sau)
- Khi train, ta cÃ³ toÃ n bá»™ data â†’ BiLSTM táº­n dá»¥ng Ä‘Æ°á»£c thÃ´ng tin nÃ y

---

## Sliding Window

### Giáº£i thÃ­ch ML
- Chia dá»¯ liá»‡u thÃ nh cÃ¡c sequences (chuá»—i) cÃ³ Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh
- Má»—i sequence dÃ¹ng Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ tiáº¿p theo

### VÃ­ dá»¥ Ä‘á»i sá»‘ng: Dá»± Ä‘oÃ¡n thá»i tiáº¿t

Báº¡n muá»‘n dá»± Ä‘oÃ¡n thá»i tiáº¿t ngÃ y mai.

**CÃ¡ch 1: Chá»‰ nhÃ¬n ngÃ y hÃ´m nay**
- HÃ´m nay: 25Â°C, náº¯ng
- Dá»± Ä‘oÃ¡n: NgÃ y mai 26Â°C, náº¯ng
- â†’ KhÃ´ng chÃ­nh xÃ¡c, chá»‰ nhÃ¬n 1 ngÃ y

**CÃ¡ch 2: Sliding Window - nhÃ¬n 7 ngÃ y trÆ°á»›c**
- Thá»© 2: 22Â°C, mÆ°a
- Thá»© 3: 23Â°C, mÆ°a
- ...
- Chá»§ nháº­t: 26Â°C, náº¯ng
- Window size = 7: Báº¡n nhÃ¬n 7 ngÃ y trÆ°á»›c â†’ Dá»± Ä‘oÃ¡n ngÃ y mai
- â†’ ChÃ­nh xÃ¡c hÆ¡n nhiá»u!

### Trong dá»± Ä‘oÃ¡n giÃ¡ Bitcoin
- Window size = 60: Model nhÃ¬n giÃ¡ 60 ngÃ y trÆ°á»›c Ä‘á»ƒ dá»± Ä‘oÃ¡n ngÃ y 61
- Window size = 30: Model nhÃ¬n giÃ¡ 30 ngÃ y trÆ°á»›c Ä‘á»ƒ dá»± Ä‘oÃ¡n ngÃ y 31

---

## Scaling (Chuáº©n HÃ³a Dá»¯ Liá»‡u)

### Giáº£i thÃ­ch ML
- ÄÆ°a dá»¯ liá»‡u vá» khoáº£ng [0, 1] hoáº·c [-1, 1]
- GiÃºp model há»c nhanh vÃ  á»•n Ä‘á»‹nh hÆ¡n

### VÃ­ dá»¥ Ä‘á»i sá»‘ng: Äá»•i tiá»n sang tá»· lá»‡

**KhÃ´ng scaling:**
- GiÃ¡ nhÃ : $1,000,000
- LÆ°Æ¡ng: $5,000
- Chi phÃ­ Äƒn uá»‘ng: $500
- â†’ Sá»‘ quÃ¡ chÃªnh lá»‡ch, khÃ³ so sÃ¡nh

**Scaling (min-max):**
- GiÃ¡ nhÃ : 1.0 (cao nháº¥t)
- LÆ°Æ¡ng: 0.5
- Chi phÃ­ Äƒn uá»‘ng: 0.0 (tháº¥p nháº¥t)
- â†’ Dá»… so sÃ¡nh, dá»… hiá»ƒu

### Táº¡i sao pháº£i scale?
- Náº¿u khÃ´ng scale: $50,000 vÃ  $51,000 gáº§n giá»‘ng nhau, nhÆ°ng model khÃ³ phÃ¢n biá»‡t
- Náº¿u scale: 0.5 vÃ  0.51 â†’ model dá»… tháº¥y sá»± khÃ¡c biá»‡t

---

## Train/Validation/Test Split

### Giáº£i thÃ­ch ML
- Chia dá»¯ liá»‡u thÃ nh 3 pháº§n Ä‘á»ƒ train, validate, vÃ  Ä‘Ã¡nh giÃ¡ model

### VÃ­ dá»¥ Ä‘á»i sá»‘ng: Há»c thi Ä‘áº¡i há»c

**Train (80%):**
- Báº¡n há»c bÃ i á»Ÿ nhÃ 
- LÃ m bÃ i táº­p, Ä‘á»c sÃ¡ch
- LÃ m xong Ä‘Ã¡p Ã¡n ngay â†’ biáº¿t mÃ¬nh Ä‘Ãºng/sai

**Validation (10%):**
- LÃ m Ä‘á» thá»­ táº¡i trÆ°á»ng
- KhÃ´ng cÃ³ Ä‘Ã¡p Ã¡n ngay, chá» tháº§y cháº¥m
- Äiá»u chá»‰nh cÃ¡ch há»c dá»±a trÃªn káº¿t quáº£

**Test (10%):**
- Thi tháº­t (Ä‘áº¡i há»c, Há»c viá»‡n, v.v.)
- ChÆ°a tá»«ng tháº¥y Ä‘á» nÃ y trÆ°á»›c
- Káº¿t quáº£ thi: Chá»‰ cÃ³ 1 láº§n!

### Táº¡i sao cáº§n 3 pháº§n?
- Train: Äá»ƒ há»c pattern
- Validation: Äá»ƒ Ä‘iá»u chá»‰nh cÃ¡ch há»c (tÄƒng epochs, giáº£m learning rate, v.v.)
- Test: Äá»ƒ biáº¿t model thá»±c chiáº¿n Ä‘Æ°á»£c khÃ´ng (chá»‰ dÃ¹ng 1 láº§n!)

---

## Epochs

### Giáº£i thÃ­ch ML
- Sá»‘ láº§n model há»c qua toÃ n bá»™ dá»¯ liá»‡u

### VÃ­ dá»¥ Ä‘á»i sá»‘ng: Äá»c má»™t cuá»‘n sÃ¡ch

**Epoch 1:**
- Äá»c cuá»‘n sÃ¡ch láº§n Ä‘áº§u tiÃªn
- Hiá»ƒu sÆ¡ sÃ i, nhá»› vÃ i Ä‘iá»ƒm chÃ­nh

**Epoch 2:**
- Äá»c láº¡i láº§n 2
- Hiá»ƒu rÃµ hÆ¡n, nhá»› nhiá»u chi tiáº¿t hÆ¡n

**Epoch 20:**
- Äá»c láº§n thá»© 20
- Hiá»ƒu ráº¥t sÃ¢u, nhá»› tá»«ng chi tiáº¿t

### Táº¡i sao cáº§n nhiá»u epochs?
- Model há»c láº§n Ä‘áº§u â†’ chÆ°a hiá»ƒu pattern cá»§a data
- Model há»c láº¡i â†’ hiá»ƒu rÃµ hÆ¡n
- Model há»c nhiá»u láº§n â†’ hiá»ƒu ráº¥t sÃ¢u

### NhÆ°ng bao nhiÃªu lÃ  Ä‘á»§?
- QuÃ¡ Ã­t epochs â†’ underfitting (khÃ´ng hiá»ƒu háº¿t)
- QuÃ¡ nhiá»u epochs â†’ overfitting (há»c váº¹t)

---

## Batch Size

### Giáº£i thÃ­ch ML
- Sá»‘ samples má»—i láº§n tÃ­nh gradient (cáº­p nháº­t weights)

### VÃ­ dá»¥ Ä‘á»i sá»‘ng: Há»c tá»« vá»±ng tiáº¿ng Anh

**Batch size = 1 (Online learning):**
- Há»c 1 tá»« â†’ kiá»ƒm tra â†’ Ä‘iá»u chá»‰nh cÃ¡ch há»c
- Há»c tiáº¿p tá»« tiáº¿p theo â†’ kiá»ƒm tra â†’ Ä‘iá»u chá»‰nh
- â†’ Há»c ráº¥t cháº­m nhÆ°ng cáº­p nháº­t liÃªn tá»¥c

**Batch size = 32 (Mini-batch learning):**
- Há»c 32 tá»« â†’ kiá»ƒm tra 32 tá»« â†’ Ä‘iá»u chá»‰nh
- Há»c tiáº¿p 32 tá»« â†’ kiá»ƒm tra â†’ Ä‘iá»u chá»‰nh
- â†’ Há»c vá»«a pháº£i, cÃ¢n báº±ng

**Batch size = 1000 (Batch learning):**
- Há»c 1000 tá»« â†’ kiá»ƒm tra 1000 tá»« â†’ Ä‘iá»u chá»‰nh
- â†’ Há»c nhanh nhÆ°ng cÃ³ thá»ƒ bá» qua chi tiáº¿t nhá»

### Trade-off:
- Batch size nhá»: Cháº­m nhÆ°ng chÃ­nh xÃ¡c
- Batch size lá»›n: Nhanh nhÆ°ng cÃ³ thá»ƒ kÃ©m chÃ­nh xÃ¡c

---

## Learning Rate

### Giáº£i thÃ­ch ML
- BÆ°á»›c nháº£y khi cáº­p nháº­t weights (Ä‘iá»u chá»‰nh tham sá»‘ model)

### VÃ­ dá»¥ Ä‘á»i sá»‘ng: TÃ¬m Ä‘Æ°á»ng lÃªn Ä‘á»‰nh nÃºi trong sÆ°Æ¡ng mÃ¹

**Learning rate lá»›n (0.1):**
- BÆ°á»›c nháº£y lá»›n
- CÃ³ thá»ƒ Ä‘Ã­ch Ä‘áº¿n nhanh
- NhÆ°ng cÃ³ thá»ƒ nháº£y quÃ¡ Ä‘Ã­ch, nháº£y xuá»‘ng vÃ¡ch Ä‘Ã¡!

**Learning rate nhá» (0.0001):**
- BÆ°á»›c nháº£y nhá», cáº©n tháº­n
- Cháº¯c cháº¯n Ä‘áº¿n Ä‘Ã­ch
- NhÆ°ng ráº¥t lÃ¢u, má»‡t má»i

**Learning rate vá»«a pháº£i (0.001):**
- BÆ°á»›c nháº£y vá»«a pháº£i
- Äáº¿n Ä‘Ã­ch nhanh mÃ  an toÃ n

---

## Dropout

### Giáº£i thÃ­ch ML
- Bá» ngáº«u nhiÃªn má»™t sá»‘ neurons trong quÃ¡ trÃ¬nh training
- GiÃºp trÃ¡nh overfitting (há»c váº¹t)

### VÃ­ dá»¥ Ä‘á»i sá»‘ng: Há»c trong nhÃ³m

**KhÃ´ng dropout (overfitting):**
- CÃ¹ng 1 nhÃ³m há»c má»i lÃºc
- Nhá»› chÃ­nh xÃ¡c ai tráº£ lá»i cÃ¢u gÃ¬
- Äáº¿n thi cÃ³ nhÃ³m Ä‘Ã³ â†’ Ä‘Æ°á»£c 10 Ä‘iá»ƒm
- Äáº¿n thi khÃ´ng cÃ³ nhÃ³m Ä‘Ã³ â†’ rá»›t!

**CÃ³ dropout:**
- Thay Ä‘á»•i thÃ nh viÃªn nhÃ³m liÃªn tá»¥c
- Há»c cÃ¡ch há»c, khÃ´ng chá»‰ nhá»› Ä‘Ã¡p Ã¡n
- Äáº¿n thi báº¥t ká»³ ai â†’ Ä‘á»u lÃ m tá»‘t

### Táº¡i sao Dropout giÃºp trÃ¡nh overfitting?
- Model khÃ´ng thá»ƒ dá»±a vÃ o má»™t vÃ i neurons cá»¥ thá»ƒ
- Buá»™c model há»c pattern chung, khÃ´ng há»c váº¹t

---

## Overfitting vs Underfitting

### VÃ­ dá»¥ Ä‘á»i sá»‘ng: Há»c thi toÃ¡n

**Underfitting (Há»c quÃ¡ Ã­t):**
- Chá»‰ há»c sÆ¡ lÆ°á»£c cÃ´ng thá»©c
- Thi â†’ khÃ´ng lÃ m Ä‘Æ°á»£c bÃ i khÃ³
- Biá»ƒu Ä‘á»“: Train loss cao, Val loss cao

**Tá»‘t (Fit vá»«a pháº£i):**
- Há»c vá»«a pháº£i, hiá»ƒu cÃ´ng thá»©c + cÃ¡ch Ã¡p dá»¥ng
- Thi â†’ lÃ m Ä‘Æ°á»£c cáº£ bÃ i dá»… vÃ  bÃ i khÃ³
- Biá»ƒu Ä‘á»“: Train loss tháº¥p, Val loss tháº¥p

**Overfitting (Há»c váº¹t):**
- Há»c váº¹t má»i Ä‘á» thi cÅ©
- Thi cÃ³ Ä‘á» giá»‘ng Ä‘á» cÅ© â†’ Ä‘Æ°á»£c 10 Ä‘iá»ƒm
- Thi cÃ³ Ä‘á» má»›i â†’ rá»›t
- Biá»ƒu Ä‘á»“: Train loss tháº¥p, Val loss cao

---

## Loss Function

### Giáº£i thÃ­ch ML
- HÃ m Ä‘o lÆ°á»ng Ä‘á»™ sai lá»‡ch giá»¯a dá»± Ä‘oÃ¡n vÃ  thá»±c táº¿
- Model cá»‘ gáº¯ng giáº£m loss cÃ ng nhá» cÃ ng tá»‘t

### VÃ­ dá»¥ Ä‘á»i sá»‘ng: Báº¯n cung

**MSE (Mean Squared Error):**
- Äiá»ƒm: (khoáº£ng cÃ¡chÂ²) â†’ Máº¥t Ä‘iá»ƒm náº·ng hÆ¡n náº¿u báº¯n lá»‡ch nhiá»u
- Báº¯n lá»‡ch 1cm â†’ máº¥t 1 Ä‘iá»ƒm
- Báº¯n lá»‡ch 10cm â†’ máº¥t 100 Ä‘iá»ƒm
- â†’ Nháº¥n máº¡nh vÃ o cÃ¡c lá»—i lá»›n

**MAE (Mean Absolute Error):**
- Äiá»ƒm: khoáº£ng cÃ¡ch â†’ Máº¥t Ä‘iá»ƒm Ä‘á»u
- Báº¯n lá»‡ch 1cm â†’ máº¥t 1 Ä‘iá»ƒm
- Báº¯n lá»‡ch 10cm â†’ máº¥t 10 Ä‘iá»ƒm
- â†’ Dá»… hiá»ƒu hÆ¡n

---

## Callbacks

### Giáº£i thÃ­ch ML
- CÃ¡c functions Ä‘Æ°á»£c gá»i trong quÃ¡ trÃ¬nh training
- GiÃºp Ä‘iá»u chá»‰nh vÃ  kiá»ƒm soÃ¡t training

### VÃ­ dá»¥ Ä‘á»i sá»‘ng: Huáº¥n luyá»‡n viÃªn theo dÃµi váº­n Ä‘á»™ng viÃªn

**ModelCheckpoint:**
- LÆ°u láº¡i ká»· lá»¥c tá»‘t nháº¥t
- Vá» sau cÃ³ thá»ƒ load láº¡i ká»· lá»¥c nÃ y

**EarlyStopping:**
- Náº¿u váº­n Ä‘á»™ng viÃªn khÃ´ng cáº£i thiá»‡n sau N láº§n táº­p â†’ dá»«ng
- Tiáº¿t kiá»‡m thá»i gian, trÃ¡nh overtraining

**ReduceLROnPlateau:**
- Náº¿u khÃ´ng cáº£i thiá»‡n â†’ giáº£m cÆ°á»ng Ä‘á»™ táº­p
- GiÃºp "fine-tune" tá»‘t hÆ¡n

---

## Metrics (MAE, RMSE, MAPE)

### VÃ­ dá»¥ Ä‘á»i sá»‘ng: ÄÃ¡nh giÃ¡ dá»± bÃ¡o thá»i tiáº¿t

**MAE (Mean Absolute Error):**
- Sai sá»‘ trung bÃ¬nh tuyá»‡t Ä‘á»‘i
- Dá»± bÃ¡o sai trung bÃ¬nh 2Â°C
- Dá»… hiá»ƒu: "BÃ¬nh thÆ°á»ng mÃ¬nh sai 2 Ä‘á»™ thÃ´i"

**RMSE (Root Mean Squared Error):**
- CÄƒn báº­c 2 cá»§a sai sá»‘ bÃ¬nh phÆ°Æ¡ng
- Dá»± bÃ¡o sai 2Â°C nhÆ°ng cÃ³ vÃ i ngÃ y sai 10Â°C â†’ RMSE sáº½ cao hÆ¡n
- Nháº¥n máº¡nh vÃ o cÃ¡c lá»—i lá»›n (outliers)

**MAPE (Mean Absolute Percentage Error):**
- Sai sá»‘ pháº§n trÄƒm trung bÃ¬nh
- Dá»± bÃ¡o sai trung bÃ¬nh 5%
- Äá»™c láº­p vá»›i scale: 5Â°C sai khi 20Â°C (25%) vs 5Â°C sai khi 100Â°C (5%)

---

## ğŸ¯ TÃ³m Táº¯t

| KhÃ¡i niá»‡m | VÃ­ dá»¥ Ä‘á»i sá»‘ng |
|-----------|---------------|
| BiLSTM | Äá»c cÃ¢u 2 chiá»u |
| Sliding Window | Dá»± Ä‘oÃ¡n thá»i tiáº¿t dá»±a trÃªn 7 ngÃ y trÆ°á»›c |
| Scaling | Äá»•i tiá»n sang tá»· lá»‡ |
| Train/Val/Test | Há»c á»Ÿ nhÃ , Ä‘á» thá»­, thi tháº­t |
| Epochs | Äá»c sÃ¡ch nhiá»u láº§n |
| Batch Size | Há»c tá»« vá»±ng theo nhÃ³m |
| Learning Rate | BÆ°á»›c nháº£y tÃ¬m Ä‘Æ°á»ng lÃªn Ä‘á»‰nh nÃºi |
| Dropout | Thay Ä‘á»•i nhÃ³m há»c |
| Overfitting | Há»c váº¹t Ä‘á» thi cÅ© |
| Loss | Äiá»ƒm báº¯n cung |
| Callbacks | Huáº¥n luyá»‡n viÃªn theo dÃµi |


