# ğŸ“– Survival Guide - HÆ°á»›ng Dáº«n Sá»‘ng CÃ²n

Giáº£i thÃ­ch chi tiáº¿t tá»«ng bÆ°á»›c + Troubleshooting cho cÃ¡c váº¥n Ä‘á» thÆ°á»ng gáº·p.

---

## ğŸ“‹ Chá»‰ Má»¥c

- [BÆ°á»›c 1: Äá»c dá»¯ liá»‡u tá»« CSV (local)](#bÆ°á»›c-1-Ä‘á»c-dá»¯-liá»‡u-tá»«-csv-local)
- [BÆ°á»›c 2: Xá»­ lÃ½ dá»¯ liá»‡u](#bÆ°á»›c-2-xá»­-lÃ½-dá»¯-liá»‡u)
- [BÆ°á»›c 3: XÃ¢y dá»±ng model BiLSTM](#bÆ°á»›c-3-xÃ¢y-dá»±ng-model-bilstm)
- [BÆ°á»›c 4: Training model](#bÆ°á»›c-4-training-model)
- [BÆ°á»›c 5: ÄÃ¡nh giÃ¡ & Váº½ biá»ƒu Ä‘á»“](#bÆ°á»›c-5-Ä‘Ã¡nh-giÃ¡--váº½-biá»ƒu-Ä‘á»“)
- [Troubleshooting](#troubleshooting)

---

## BÆ°á»›c 1: Äá»c Dá»¯ Liá»‡u Tá»« CSV (Local)

### Giáº£i thÃ­ch
- **fetch_binance_data()**: (giá»¯ tÃªn cÅ© cho tÆ°Æ¡ng thÃ­ch) nhÆ°ng thá»±c táº¿ lÃ  **Ä‘á»c file CSV local**
- **Dá»¯ liá»‡u máº·c Ä‘á»‹nh**: `data/btc_15m_data_2018_to_2025.csv` (táº­p trung 15m)
- **Cache**: LÆ°u file CSV Ä‘Ã£ chuáº©n hoÃ¡ (datetime/open/high/low/close/volume) vÃ o `data/cache/` Ä‘á»ƒ láº§n sau Ä‘á»c nhanh hÆ¡n
- **Timeframe**: Chá»‰ dÃ¹ng Ä‘á»ƒ chá»n file máº·c Ä‘á»‹nh náº¿u khÃ´ng set `data_path` (15m/1h/4h/1d)

### CÃ¡c tham sá»‘
| Tham sá»‘ | Giáº£i thÃ­ch | Máº·c Ä‘á»‹nh |
|---------|------------|----------|
| `data_path` | ÄÆ°á»ng dáº«n CSV | data/btc_15m_data_2018_to_2025.csv |
| `timeframe` | DÃ¹ng Ä‘á»ƒ chá»n file máº·c Ä‘á»‹nh | 15m |
| `limit` | Láº¥y N dÃ²ng cuá»‘i cá»§a CSV | 50000 (cho 15m) |
| `save_cache` | CÃ³ lÆ°u cache khÃ´ng | True |

### Dá»¯ liá»‡u tráº£ vá»
DataFrame vá»›i cÃ¡c cá»™t:
- `datetime`: Thá»i gian
- `open`: GiÃ¡ má»Ÿ náº¿n
- `high`: GiÃ¡ cao nháº¥t
- `low`: GiÃ¡ tháº¥p nháº¥t
- `close`: GiÃ¡ Ä‘Ã³ng náº¿n
- `volume`: Khá»‘i lÆ°á»£ng giao dá»‹ch

### VÃ­ dá»¥ code
```python
from src.core import fetch_binance_data

df = fetch_binance_data(
    data_path="data/btc_15m_data_2018_to_2025.csv",
    timeframe="15m",
    limit=50000
)

print(df.head())
```

---

## BÆ°á»›c 2: Xá»­ LÃ½ Dá»¯ Liá»‡u

### 2.1: Scaling (Chuáº©n hÃ³a dá»¯ liá»‡u)

#### Giáº£i thÃ­ch
- **Táº¡i sao cáº§n scaling?** 
  - GiÃ¡ Bitcoin dao Ä‘á»™ng tá»« $10,000 Ä‘áº¿n $100,000
  - Sá»‘ quÃ¡ lá»›n khiáº¿n model khÃ³ há»c
  - Scaling Ä‘Æ°a táº¥t cáº£ vá» khoáº£ng [0, 1] hoáº·c [-1, 1]

- **MinMaxScaler**: 
  - ÄÆ°a dá»¯ liá»‡u vá» khoáº£ng [0, 1]
  - CÃ´ng thá»©c: (x - min) / (max - min)
  - VÃ­ dá»¥: $50,000 (trong range $10K-$100K) â†’ (50K-10K)/(100K-10K) = 0.44

- **StandardScaler**:
  - ÄÆ°a dá»¯ liá»‡u vá» mean=0, std=1
  - CÃ´ng thá»©c: (x - mean) / std
  - Tá»‘t khi data cÃ³ phÃ¢n phá»‘i Gaussian

#### DÃ¹ng scaler nÃ o?
- Vá»›i giÃ¡ crypto: DÃ¹ng **MinMaxScaler** (giÃ¡ luÃ´n > 0, ta biáº¿t min/max)
- Vá»›i data cÃ³ outliers nhiá»u: DÃ¹ng **StandardScaler**

### 2.2: Sliding Window (Táº¡o sequences)

#### Giáº£i thÃ­ch
- **Táº¡i sao cáº§n sliding window?**
  - LSTM há»c tá»« sequences (chuá»—i) chá»© khÃ´ng pháº£i tá»«ng Ä‘iá»ƒm Ä‘Æ¡n láº»
  - Äá»ƒ dá»± Ä‘oÃ¡n giÃ¡ ngÃ y mai, cáº§n nhÃ¬n giÃ¡ cá»§a 60 ngÃ y trÆ°á»›c Ä‘Ã³

- **Window size**: Sá»‘ bÆ°á»›c nhÃ¬n láº¡i (past days)
  - Window size = 60: Model nhÃ¬n 60 ngÃ y trÆ°á»›c Ä‘á»ƒ dá»± Ä‘oÃ¡n ngÃ y tiáº¿p theo
  - Window size = 30: Model nhÃ¬n 30 ngÃ y trÆ°á»›c

#### VÃ­ dá»¥
```
Dá»¯ liá»‡u: [10000, 15000, 20000, 25000, 30000, 35000, 40000]
Window size = 3

Sample 1: Input: [10000, 15000, 20000] â†’ Output: 25000
Sample 2: Input: [15000, 20000, 25000] â†’ Output: 30000
Sample 3: Input: [20000, 25000, 30000] â†’ Output: 35000
...
```

### 2.3: Split Data (Chia train/val/test)

#### Giáº£i thÃ­ch
- **Train (80%)**: DÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n model
- **Validation (10%)**: DÃ¹ng Ä‘á»ƒ Ä‘iá»u chá»‰nh hyperparameters, dá»«ng training sá»›m
- **Test (10%)**: DÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng (chá»‰ dÃ¹ng 1 láº§n!)

#### Táº¡i sao pháº£i chia?
- Náº¿u train vÃ  test trÃªn cÃ¹ng dá»¯ liá»‡u â†’ model "váº¹t", khÃ´ng thá»±c chiáº¿n Ä‘Æ°á»£c
- Test set nhÆ° "Ä‘á» thi cuá»‘i ká»³" - model chÆ°a bao giá» tháº¥y

#### LÆ°u Ã½ quan trá»ng
- âŒ KHÃ”NG shuffle data khi split (vÃ¬ lÃ  time series!)
- âœ… Pháº£i giá»¯ nguyÃªn thá»© tá»± thá»i gian

### VÃ­ dá»¥ code
```python
from src.core import prepare_data_for_lstm

data_dict = prepare_data_for_lstm(
    df=df,
    features=["close"],
    window_size=60,
    scaler_type="minmax"
)

X_train = data_dict['X_train']
y_train = data_dict['y_train']
X_val = data_dict['X_val']
y_val = data_dict['y_val']
X_test = data_dict['X_test']
y_test = data_dict['y_test']
```

---

## BÆ°á»›c 3: XÃ¢y Dá»±ng Model BiLSTM

### Giáº£i thÃ­ch
- **LSTM (Long Short-Term Memory)**:
  - RNN cáº£i tiáº¿n cÃ³ kháº£ nÄƒng ghi nhá»› thÃ´ng tin dÃ i háº¡n
  - Giáº£i quyáº¿t vanishing gradient problem cá»§a RNN thÆ°á»ng

- **BiLSTM (Bidirectional LSTM)**:
  - LSTM nhÃ¬n cáº£ quÃ¡ khá»© VÃ€ tÆ°Æ¡ng lai
  - 2 LSTM: 1 Ä‘á»c tá»« trÃ¡i â†’ pháº£i, 1 Ä‘á»c tá»« pháº£i â†’ trÃ¡i
  - Táº¡i sao nhÃ¬n tÆ°Æ¡ng lai Ä‘Æ°á»£c? VÃ¬ khi train, ta cÃ³ toÃ n bá»™ data!

- **Dropout**:
  - Bá» ngáº«u nhiÃªn má»™t sá»‘ neurons trong quÃ¡ trÃ¬nh training
  - GiÃºp trÃ¡nh overfitting (model há»c váº¹t)

- **Dense layers**:
  - Layers káº¿t ná»‘i Ä‘áº§y Ä‘á»§
  - Káº¿t há»£p cÃ¡c features Ä‘Ã£ há»c Ä‘Æ°á»£c tá»« LSTM Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n

### CÃ¡c tham sá»‘ quan trá»ng
| Tham sá»‘ | Giáº£i thÃ­ch | Máº·c Ä‘á»‹nh | Khuyáº¿n nghá»‹ |
|---------|------------|----------|-------------|
| `lstm_units` | Sá»‘ neurons trong má»—i LSTM layer | [64, 32] | 32-128 |
| `dropout_rate` | Tá»· lá»‡ bá» neurons | 0.2 | 0.1-0.5 |
| `dense_units` | Sá»‘ neurons trong Dense layers | [16] | 8-64 |

### CÃ¡ch chá»n sá»‘ layers & units?
- **Data Ã­t (< 1000 samples)**: 1-2 LSTM layers, 16-32 units
- **Data vá»«a (1000-10000 samples)**: 2-3 LSTM layers, 32-64 units
- **Data nhiá»u (> 10000 samples)**: 3+ LSTM layers, 64-128 units

### VÃ­ dá»¥ code
```python
from src.core import build_bilstm_model

input_shape = (60, 1)  # (window_size, n_features)
model = build_bilstm_model(
    input_shape=input_shape,
    lstm_units=[64, 32],
    dropout_rate=0.2,
    dense_units=[16],
    output_units=1
)
```

---

## BÆ°á»›c 4: Training Model

### Giáº£i thÃ­ch
- **Epochs**: Sá»‘ láº§n model há»c qua toÃ n bá»™ dá»¯ liá»‡u
  - Epoch 1: Model há»c láº§n Ä‘áº§u, chÆ°a hiá»ƒu nhiá»u
  - Epoch 2: Model há»c láº¡i, hiá»ƒu rÃµ hÆ¡n
  - Epoch 20: Model Ä‘Ã£ hiá»ƒu tá»‘t pattern cá»§a dá»¯ liá»‡u

- **Batch size**: Sá»‘ samples má»—i láº§n tÃ­nh gradient
  - Batch size nhá» â†’ train cháº­m nhÆ°ng chÃ­nh xÃ¡c hÆ¡n
  - Batch size lá»›n â†’ train nhanh nhÆ°ng cÃ³ thá»ƒ kÃ©m chÃ­nh xÃ¡c hÆ¡n

- **Learning rate**: BÆ°á»›c nháº£y khi cáº­p nháº­t weights
  - LR lá»›n â†’ há»c nhanh nhÆ°ng cÃ³ thá»ƒ "nháº£y qua" optimum
  - LR nhá» â†’ há»c cháº­m nhÆ°ng chÃ­nh xÃ¡c hÆ¡n

### Callbacks lÃ  gÃ¬?

#### 1. ModelCheckpoint
- LÆ°u láº¡i model tá»‘t nháº¥t (cÃ³ val_loss tháº¥p nháº¥t)
- Vá» sau cÃ³ thá»ƒ load láº¡i mÃ  khÃ´ng cáº§n train láº¡i

#### 2. EarlyStopping
- Dá»«ng training khi val_loss khÃ´ng giáº£m sau N epochs
- Tiáº¿t kiá»‡m thá»i gian, trÃ¡nh overfitting

#### 3. ReduceLROnPlateau
- Giáº£m learning rate khi val_loss khÃ´ng giáº£m
- GiÃºp model "fine-tune" tá»‘t hÆ¡n

### VÃ­ dá»¥ code
```python
from src.training import train_model
from src import Config

# Táº¡o Config object
config = Config()
config.training.epochs = 20
config.training.batch_size = 32
config.training.early_stopping_patience = 5

train_result = train_model(
    model=model,
    X_train=X_train,
    y_train=y_train,
    X_val=X_val,
    y_val=y_val,
    config=config
)

history = train_result['history']
```

---

## BÆ°á»›c 5: ÄÃ¡nh GiÃ¡ & Váº½ Biá»ƒu Äá»“

### Metrics lÃ  gÃ¬?

#### MAE (Mean Absolute Error)
- **CÃ´ng thá»©c**: mean(|y_true - y_pred|)
- **Giáº£i thÃ­ch**: Sai sá»‘ trung bÃ¬nh tuyá»‡t Ä‘á»‘i
- **VÃ­ dá»¥**: MAE = $500 â†’ Model dá»± Ä‘oÃ¡n sai trung bÃ¬nh $500
- **Æ¯u Ä‘iá»ƒm**: Dá»… hiá»ƒu

#### RMSE (Root Mean Squared Error)
- **CÃ´ng thá»©c**: sqrt(mean((y_true - y_pred)Â²))
- **Giáº£i thÃ­ch**: CÄƒn báº­c 2 cá»§a sai sá»‘ bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh
- **Æ¯u Ä‘iá»ƒm**: Nháº¥n máº¡nh vÃ o cÃ¡c lá»—i lá»›n (outliers)

#### MAPE (Mean Absolute Percentage Error)
- **CÃ´ng thá»©c**: mean(|y_true - y_pred| / y_true) * 100
- **Giáº£i thÃ­ch**: Sai sá»‘ pháº§n trÄƒm trung bÃ¬nh
- **VÃ­ dá»¥**: MAPE = 2% â†’ Model sai trung bÃ¬nh 2%
- **Æ¯u Ä‘iá»ƒm**: Äá»™c láº­p vá»›i scale cá»§a giÃ¡

### ÄÃ¡nh giÃ¡: Káº¿t quáº£ tá»‘t hay xáº¥u?

| MAE | RMSE | MAPE | ÄÃ¡nh giÃ¡ |
|-----|------|------|----------|
| < $200 | < $300 | < 1% | ğŸ† Tuyá»‡t vá»i |
| $200-$500 | $300-$800 | 1-2% | âœ… Tá»‘t |
| $500-$1000 | $800-$1500 | 2-5% | âš ï¸ Trung bÃ¬nh |
| > $1000 | > $1500 | > 5% | âŒ KÃ©m |

### VÃ­ dá»¥ code
```python
from src.core import evaluate_model, print_sample_predictions

eval_result = evaluate_model(
    model=model,
    X_test=X_test,
    y_test=y_test,
    scaler=scaler,
    return_predictions=True
)

y_true = eval_result['y_true']
y_pred = eval_result['predictions']

print_sample_predictions(y_true, y_pred, n_samples=10)
```

---

## Troubleshooting

### âŒ Lá»—i 1: "No module named 'tensorflow'" (hoáº·c polars/numpy)

**NguyÃªn nhÃ¢n**: ChÆ°a cÃ i dependencies

**Giáº£i phÃ¡p**:
```bash
uv sync
```

---

### âŒ Lá»—i 2: "FileNotFoundError: KhÃ´ng tÃ¬m tháº¥y file data"

**NguyÃªn nhÃ¢n**: `--data-path` trá» sai, hoáº·c báº¡n chÆ°a cÃ³ file CSV trong `data/`

**Giáº£i phÃ¡p**:
- Kiá»ƒm tra file máº·c Ä‘á»‹nh: `data/btc_1d_data_2018_to_2025.csv`
- Hoáº·c chá»‰ Ä‘á»‹nh rÃµ:

```bash
uv run python -m cli.main --data-path data/btc_1d_data_2018_to_2025.csv
```

---

### âŒ Lá»—i 3: "CSV thiáº¿u cá»™t báº¯t buá»™c"

**NguyÃªn nhÃ¢n**: File CSV khÃ´ng Ä‘Ãºng format (cáº§n cÃ³ cÃ¡c cá»™t kiá»ƒu Binance export: `Open time`, `Open`, `High`, `Low`, `Close`, `Volume`)

**Giáº£i phÃ¡p**:
- DÃ¹ng Ä‘Ãºng file máº·c Ä‘á»‹nh trong `data/`
- Hoáº·c sá»­a header CSV cho khá»›p cÃ¡c cá»™t trÃªn

---

### âŒ Lá»—i 4: Overfitting (Train loss tháº¥p, Val loss cao)

**NguyÃªn nhÃ¢n**: Model há»c váº¹t data training

**Giáº£i phÃ¡p**:
1. TÄƒng dropout rate (0.2 â†’ 0.3, 0.4)
2. Giáº£m sá»‘ units trong LSTM
3. Giáº£m sá»‘ epochs
4. TÄƒng data training

---

### âŒ Lá»—i 5: Underfitting (Cáº£ train vÃ  val loss Ä‘á»u cao)

**NguyÃªn nhÃ¢n**: Model quÃ¡ Ä‘Æ¡n giáº£n, khÃ´ng há»c Ä‘Æ°á»£c pattern

**Giáº£i phÃ¡p**:
1. TÄƒng sá»‘ LSTM layers
2. TÄƒng sá»‘ units
3. TÄƒng sá»‘ epochs
4. ThÃªm cÃ¡c features khÃ¡c (volume, open, high, low)

---

### âŒ Lá»—i 5: Model khÃ´ng converge (loss dao Ä‘á»™ng)

**NguyÃªn nhÃ¢n**: Learning rate quÃ¡ lá»›n

**Giáº£i phÃ¡p**:
1. Giáº£m learning rate (máº·c Ä‘á»‹nh 0.001 â†’ 0.0005, 0.0001)
2. Sá»­ dá»¥ng ReduceLROnPlateau callback (Ä‘Ã£ báº­t máº·c Ä‘á»‹nh)

---

### âŒ Lá»—i 6: Out of Memory (OOM)

**NguyÃªn nhÃ¢n**: Batch size quÃ¡ lá»›n hoáº·c data quÃ¡ nhiá»u

**Giáº£i phÃ¡p**:
1. Giáº£m batch size (32 â†’ 16, 8)
2. Giáº£m window size
3. Giáº£m sá»‘ units trong LSTM

---

### âŒ Lá»—i 7: Káº¿t quáº£ dá»± Ä‘oÃ¡n ráº¥t kÃ©m

**NguyÃªn nhÃ¢n cÃ³ thá»ƒ**:
1. Data quÃ¡ Ã­t (< 10000 samples cho 15m)
2. Window size khÃ´ng phÃ¹ há»£p
3. Model quÃ¡ phá»©c táº¡p so vá»›i data
4. Market Ä‘ang volatile (dá»± Ä‘oÃ¡n giÃ¡ crypto ráº¥t khÃ³!)

**Giáº£i phÃ¡p**:
1. TÄƒng limit (10000 â†’ 50000 cho 15m)
2. Thá»­ preset khÃ¡c (scalping-ultra-fast â†’ intraday-balanced)
3. Thá»­ window size khÃ¡c (24 â†’ 96 â†’ 240)
4. Nháº­n ráº±ng dá»± Ä‘oÃ¡n giÃ¡ crypto lÃ  váº¥n Ä‘á» ráº¥t khÃ³!

---

### âŒ Lá»—i 8: GPU khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng

**NguyÃªn nhÃ¢n**: TensorFlow khÃ´ng tÃ¬m tháº¥y GPU

**Giáº£i phÃ¡p**:
- Vá»›i CPU AMD: ÄÃ¢y lÃ  bÃ¬nh thÆ°á»ng, project Ä‘Ã£ tá»‘i Æ°u cho CPU
- Náº¿u cÃ³ NVIDIA GPU: CÃ i CUDA, cuDNN

---

## ğŸ’¡ Tips & Tricks

1. **LuÃ´n dÃ¹ng cache**: Äá»«ng táº£i láº¡i data má»—i láº§n cháº¡y
2. **Báº¯t Ä‘áº§u vá»›i config Ä‘Æ¡n giáº£n**: 1-2 LSTM layers, 32-64 units
3. **Theo dÃµi val_loss**: KhÃ´ng chá»‰ train loss!
4. **DÃ¹ng EarlyStopping**: Tiáº¿t kiá»‡m thá»i gian
5. **Test trÃªn data tháº­t**: KhÃ´ng chá»‰ nhÃ¬n train/val metrics

---

## ğŸ¯ Káº¿t Luáº­n

Náº¿u báº¡n theo dÃµi tá»«ng bÆ°á»›c vÃ  gáº·p váº¥n Ä‘á»:
1. Äá»c láº¡i pháº§n tÆ°Æ¡ng á»©ng trong file nÃ y
2. Äá»c ANALOGIES.md Ä‘á»ƒ hiá»ƒu khÃ¡i niá»‡m
3. Xem error message vÃ  tÃ¬m trong Troubleshooting

