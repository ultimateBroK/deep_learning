"""
ğŸ‹ï¸ TRAINING MODULE - HUáº¤N LUYá»†N MODEL
-----------------------------------------

Giáº£i thÃ­ch báº±ng vÃ­ dá»¥ Ä‘á»i sá»‘ng:
- Training giá»‘ng nhÆ° "táº­p lÃ m bÃ i táº­p"
- Model há»c tá»« cÃ¡c vÃ­ dá»¥ (data) Ä‘á»ƒ tÃ¬m pattern
- Má»—i epoch = 1 láº§n há»c háº¿t toÃ n bá»™ bÃ i táº­p

Callback lÃ  gÃ¬?
- Giá»‘ng nhÆ° "ngÆ°á»i giÃ¡m sÃ¡t" trong quÃ¡ trÃ¬nh training
- EarlyStopping: Dá»«ng láº¡i khi model khÃ´ng cÃ²n há»c Ä‘Æ°á»£c gÃ¬
- ModelCheckpoint: LÆ°u láº¡i model tá»‘t nháº¥t
- ReduceLROnPlateau: Giáº£m learning rate khi model khÃ´ng cÃ²n tiáº¿n bá»™

TrÃ¡ch nhiá»‡m (SoC):
- Chá»‰ handle training logic
- KhÃ´ng chá»©a business logic khÃ¡c
"""

import time
from pathlib import Path
from typing import Any, Dict, Optional

import numpy as np
from tensorflow import keras
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

from .config import Config


def train_model(
    model: keras.Model,
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_val: np.ndarray,
    y_val: np.ndarray,
    config: Config
) -> Dict[str, Any]:
    """
    Huáº¥n luyá»‡n model vá»›i callbacks

    Args:
        model: Model Ä‘Ã£ Ä‘Æ°á»£c build
        X_train, y_train: Dá»¯ liá»‡u train
        X_val, y_val: Dá»¯ liá»‡u validation
        config: Cáº¥u hÃ¬nh

    Returns:
        Dictionary chá»©a:
            - history: Training history
            - best_epoch: Epoch cÃ³ val_loss tháº¥p nháº¥t
            - train_seconds: Thá»i gian training
            - checkpoint_path: ÄÆ°á»ng dáº«n checkpoint
    """
    # Táº¡o thÆ° má»¥c checkpoint
    checkpoint_dir = config.paths.models_dir / "checkpoints"
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    checkpoint_path = checkpoint_dir / "best_model.keras"

    # Callbacks
    checkpoint_callback = ModelCheckpoint(
        filepath=str(checkpoint_path),
        monitor='val_loss',
        save_best_only=True,
        verbose=1,
        mode='min'
    )

    early_stop_callback = EarlyStopping(
        monitor='val_loss',
        patience=config.training.early_stopping_patience,
        restore_best_weights=True,
        verbose=1,
        mode='min'
    )

    reduce_lr_callback = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=1e-6,
        verbose=1
    )

    callbacks = [checkpoint_callback, early_stop_callback, reduce_lr_callback]

    print("\n" + "=" * 70)
    print("ğŸš€ Báº®T Äáº¦U TRAINING")
    print("=" * 70)
    print(f"Epochs: {config.training.epochs}")
    print(f"Batch size: {config.training.batch_size}")
    print(f"Train samples: {len(X_train)}")
    print(f"Val samples: {len(X_val)}")
    print(f"Checkpoint: {checkpoint_path}")
    print("=" * 70 + "\n")

    t0 = time.perf_counter()
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=config.training.epochs,
        batch_size=config.training.batch_size,
        callbacks=callbacks,
        verbose=1
    )
    train_seconds = time.perf_counter() - t0

    best_epoch = np.argmin(history.history['val_loss']) + 1
    best_val_loss = min(history.history['val_loss'])

    print("\n" + "=" * 70)
    print("âœ… TRAINING HOÃ€N THÃ€NH")
    print("=" * 70)
    print(f"Best epoch: {best_epoch}/{config.training.epochs}")
    print(f"Best val_loss: {best_val_loss:.6f}")
    print(f"Best val_mae: {history.history['val_mae'][best_epoch-1]:.6f}")
    print(f"Training time: {train_seconds:.2f}s")
    print("=" * 70 + "\n")

    return {
        "history": history,
        "best_epoch": best_epoch,
        "best_val_loss": best_val_loss,
        "train_seconds": train_seconds,
        "checkpoint_path": str(checkpoint_path)
    }


def load_checkpoint(checkpoint_path: str) -> keras.Model:
    """
    Load model tá»« checkpoint

    Args:
        checkpoint_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file .keras hoáº·c .h5

    Returns:
        Model Ä‘Ã£ load
    """
    model = keras.models.load_model(checkpoint_path)
    print(f"âœ… ÄÃ£ load model tá»«: {checkpoint_path}")
    return model


def clean_checkpoints(
    config: Optional[Config] = None,
    keep_best: bool = True
) -> int:
    """
    XÃ³a cÃ¡c checkpoint cÅ©

    Args:
        config: Cáº¥u hÃ¬nh
        keep_best: CÃ³ giá»¯ láº¡i checkpoint "best" khÃ´ng

    Returns:
        Sá»‘ file Ä‘Ã£ xÃ³a
    """
    if config is None:
        config = Config()

    checkpoint_dir = config.paths.models_dir / "checkpoints"

    if not checkpoint_dir.exists():
        return 0

    deleted_count = 0

    for file_path in checkpoint_dir.glob("*.keras"):
        if keep_best and "best" in file_path.name.lower():
            continue

        file_path.unlink()
        deleted_count += 1

    if deleted_count > 0:
        print(f"ğŸ—‘ï¸  ÄÃ£ xÃ³a {deleted_count} checkpoint")
    else:
        print("âœ… KhÃ´ng cÃ³ checkpoint nÃ o Ä‘á»ƒ xÃ³a")

    return deleted_count


if __name__ == "__main__":
    print("Training module loaded successfully!")
