"""
‚öôÔ∏è TENSORFLOW RUNTIME CONFIGURATION
-------------------------------------

Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
- Gi·ªëng nh∆∞ "c√†i ƒë·∫∑t game" - c·∫•u h√¨nh ƒë·ªÉ ch·∫°y m∆∞·ª£t
- V·ªõi CPU AMD, c·∫ßn c·∫•u h√¨nh s·ªë threads ƒë·ªÉ t·ªëi ∆∞u
- Gi·ªëng nh∆∞ b·∫°n c√≥ 12 nh√¢n CPU, n√™n t·∫≠n d·ª•ng h·∫øt

L∆∞u √Ω:
- intra_op_parallelism_threads: S·ªë thread cho c√°c operations song song
- inter_op_parallelism_threads: S·ªë thread cho c√°c operations song song kh√°c
- enable_xla: T·ªëi ∆∞u code b·∫±ng XLA (Accelerated Linear Algebra)
"""

import os
import random
import warnings
from typing import Optional

import numpy as np

# Suppress warnings TR∆Ø·ªöC khi import TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Ch·ªâ hi·ªÉn th·ªã ERROR v√† WARNING nghi√™m tr·ªçng
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # T·∫Øt oneDNN warnings

# Suppress FutureWarning v·ªÅ np.object
warnings.filterwarnings('ignore', category=FutureWarning, message='.*np.object.*')
warnings.filterwarnings('ignore', message='.*oneDNN.*')
warnings.filterwarnings('ignore', message='.*CUDA.*')

import tensorflow as tf

# Suppress TensorFlow warnings sau khi import
tf.get_logger().setLevel('ERROR')


def set_random_seed(seed: int, deterministic: bool = False) -> None:
    """
    C·ªë ƒë·ªãnh ng·∫´u nhi√™n ƒë·ªÉ k·∫øt qu·∫£ ch·∫°y c√≥ th·ªÉ t√°i l·∫≠p

    Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
    - Gi·ªëng nh∆∞ "seed trong game" - gi·ªëng nhau ‚Üí gi·ªëng nhau
    - N·∫øu seed = 42, l·∫ßn n√†o ch·∫°y c≈©ng ra k·∫øt qu·∫£ gi·ªëng nhau

    Args:
        seed: S·ªë seed (v√≠ d·ª• 42). N·∫øu seed < 0 th√¨ kh√¥ng l√†m g√¨.
        deterministic: C·ªë g·∫Øng b·∫≠t deterministic ops (best-effort)
    """
    if seed is None or seed < 0:
        return

    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)

    try:
        tf.keras.utils.set_random_seed(seed)
    except Exception:
        tf.random.set_seed(seed)

    if deterministic:
        os.environ.setdefault("TF_DETERMINISTIC_OPS", "1")
        try:
            tf.config.experimental.enable_op_determinism()
        except Exception:
            pass


def configure_tensorflow_runtime(
    intra_op_threads: int = 12,
    inter_op_threads: int = 2,
    enable_xla: bool = True,
    use_gpu: bool = False
) -> None:
    """
    C·∫•u h√¨nh runtime TensorFlow cho CPU AMD

    Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
    - Gi·ªëng nh∆∞ "c·∫•u h√¨nh game" - CPU d√πng m·∫•y thread?
    - Intra-op: C√°c operations trong 1 layer song song
    - Inter-op: C√°c layer kh√°c nhau song song

    Args:
        intra_op_threads: S·ªë thread cho operations song song (s·ªë core v·∫≠t l√Ω)
        inter_op_threads: S·ªë thread cho operations song song kh√°c
        enable_xla: B·∫≠t XLA optimization
        use_gpu: C√≥ d√πng GPU kh√¥ng
    """
    old_level = tf.get_logger().level
    tf.get_logger().setLevel('ERROR')

    try:
        # C·∫•u h√¨nh s·ªë threads
        tf.config.threading.set_intra_op_parallelism_threads(intra_op_threads)
        tf.config.threading.set_inter_op_parallelism_threads(inter_op_threads)

        # B·∫≠t XLA optimization
        if enable_xla:
            os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'

        # GPU handling
        if not use_gpu:
            # Ch·ªâ ch·∫°y tr√™n CPU (·∫©n GPU n·∫øu c√≥)
            try:
                gpus = tf.config.list_physical_devices('GPU')
                if gpus:
                    tf.config.set_visible_devices([], 'GPU')
            except Exception:
                pass  # Suppress error message
    finally:
        tf.get_logger().setLevel(old_level)

    # In th√¥ng tin c·∫•u h√¨nh
    print("=" * 60)
    print("‚öôÔ∏è  C·∫§U H√åNH TENSORFLOW RUNTIME")
    print("=" * 60)
    print(f"Intra-op threads: {intra_op_threads}")
    print(f"Inter-op threads: {inter_op_threads}")
    print(f"XLA enabled: {enable_xla}")
    print(f"Use GPU: {use_gpu}")
    print("=" * 60 + "\n")


def get_gpu_info() -> bool:
    """
    Ki·ªÉm tra GPU c√≥ s·∫µn kh√¥ng

    Returns:
        True n·∫øu c√≥ GPU, False n·∫øu kh√¥ng
    """
    old_level = tf.get_logger().level
    tf.get_logger().setLevel('ERROR')

    try:
        gpus = tf.config.list_physical_devices('GPU')
    finally:
        tf.get_logger().setLevel(old_level)

    if gpus:
        print(f"‚úÖ T√¨m th·∫•y {len(gpus)} GPU:")
        for gpu in gpus:
            print(f"   - {gpu.name}")
        return True
    else:
        print("‚ÑπÔ∏è  Kh√¥ng t√¨m th·∫•y GPU, s·∫Ω d√πng CPU")
        return False


def set_memory_growth() -> None:
    """
    Cho ph√©p GPU t·ª± tƒÉng b·ªô nh·ªõ khi c·∫ßn

    Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
    - Gi·ªëng nh∆∞ "l·ª±a ph√≤ng" - chi·∫øm l√∫c c·∫ßn, kh√¥ng ph·∫£i l√∫c n√†o c≈©ng chi·∫øm h·∫øt
    - Tr√°nh chi·∫øm h·∫øt VRAM ngay t·ª´ ƒë·∫ßu
    """
    gpus = tf.config.list_physical_devices('GPU')

    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print("‚úÖ ƒê√£ b·∫≠t memory growth cho GPU")
        except RuntimeError as e:
            print(f"‚ùå L·ªói khi c·∫•u h√¨nh GPU: {e}")


def print_tensorflow_info() -> None:
    """
    In th√¥ng tin v·ªÅ TensorFlow v√† runtime
    """
    print("\n" + "=" * 60)
    print("üìã TH√îNG TIN TENSORFLOW")
    print("=" * 60)
    print(f"TensorFlow version: {tf.__version__}")
    print(f"Keras version: {tf.keras.__version__}")
    print(f"Built with CUDA: {tf.test.is_built_with_cuda()}")
    print(f"GPU available: {get_gpu_info()}")

    # CPU threads
    print(f"Intra-op threads: {tf.config.threading.get_intra_op_parallelism_threads()}")
    print(f"Inter-op threads: {tf.config.threading.get_inter_op_parallelism_threads()}")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    # Test functions
    configure_tensorflow_runtime()
    print_tensorflow_info()
