"""
ğŸ§  MODEL MODULE - XÃ‚Y Dá»°NG MODEL BiLSTM
------------------------------------------

Giáº£i thÃ­ch báº±ng vÃ­ dá»¥ Ä‘á»i sá»‘ng:
- LSTM giá»‘ng nhÆ° "bá»™ nhá»› ngáº¯n háº¡n" - nhá»› thÃ´ng tin quan trá»ng
- BiLSTM (Bidirectional) nhÃ¬n cáº£ quÃ¡ khá»© VÃ€ tÆ°Æ¡ng lai
- Táº¡i sao nhÃ¬n tÆ°Æ¡ng lai Ä‘Æ°á»£c? VÃ¬ khi train, ta cÃ³ toÃ n bá»™ dá»¯ liá»‡u!

VÃ­ dá»¥ Ä‘á»i sá»‘ng:
- LSTM thÆ°á»ng: "NhÃ¬n báº£n Ä‘á»“ tá»« Ä‘iá»ƒm A Ä‘áº¿n Ä‘iá»ƒm B"
- BiLSTM: "NhÃ¬n báº£n Ä‘á»“ tá»« A Ä‘áº¿n B + B Ä‘áº¿n A" â†’ hiá»ƒu rÃµ hÆ¡n

Lá»£i Ã­ch:
- PhÃ¡t hiá»‡n pattern tá»‘t hÆ¡n
- Hiá»ƒu context tá»« 2 phÃ­a
- Káº¿t quáº£ thÆ°á»ng tá»‘t hÆ¡n LSTM thÆ°á»ng
"""

from typing import List, Tuple

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models


def build_bilstm_model(
    input_shape: Tuple[int, int],
    lstm_units: List[int] = None,
    dropout_rate: float = 0.2,
    dense_units: List[int] = None,
    output_units: int = 1,
    learning_rate: float = 0.001
) -> models.Sequential:
    """
    XÃ¢y dá»±ng model BiLSTM

    Args:
        input_shape: Shape cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o (window_size, n_features)
        lstm_units: List sá»‘ units cho má»—i LSTM layer
        dropout_rate: Tá»· lá»‡ dropout (Ä‘á»ƒ trÃ¡nh overfitting)
        dense_units: List sá»‘ units cho má»—i Dense layer
        output_units: Sá»‘ units á»Ÿ output layer (thÆ°á»ng = 1)
        learning_rate: Learning rate cho optimizer

    Returns:
        Model BiLSTM Ä‘Ã£ Ä‘Æ°á»£c compile

    VÃ­ dá»¥:
        input_shape = (60, 1)  # 60 ngÃ y, 1 feature (giÃ¡ close)
        lstm_units = [64, 32]   # 2 LSTM layers vá»›i 64 vÃ  32 units
        output_units = 1       # Dá»± Ä‘oÃ¡n 1 giÃ¡ trá»‹ (giÃ¡ ngÃ y mai)
    """
    if lstm_units is None:
        lstm_units = [64, 32]
    if dense_units is None:
        dense_units = [16]
    
    model = models.Sequential(name="BiLSTM_Price_Prediction")

    # Input layer
    model.add(layers.Input(shape=input_shape, name="input"))

    # BiLSTM layers
    # Layer 1: return_sequences=True Ä‘á»ƒ pass cho LSTM layer tiáº¿p theo
    model.add(layers.Bidirectional(
        layers.LSTM(
            lstm_units[0],
            return_sequences=len(lstm_units) > 1,
            name="bilstm_1"
        ),
        name="bidirectional_1"
    ))
    model.add(layers.Dropout(dropout_rate, name="dropout_1"))

    # CÃ¡c LSTM layers tiáº¿p theo (náº¿u cÃ³)
    for i, units in enumerate(lstm_units[1:], start=2):
        is_last = i == len(lstm_units)
        model.add(layers.Bidirectional(
            layers.LSTM(
                units,
                return_sequences=not is_last,
                name=f"bilstm_{i}"
            ),
            name=f"bidirectional_{i}"
        ))
        model.add(layers.Dropout(dropout_rate, name=f"dropout_{i}"))

    # Dense layers
    for i, units in enumerate(dense_units, start=1):
        model.add(layers.Dense(units, activation='relu', name=f"dense_{i}"))
        model.add(layers.Dropout(dropout_rate * 0.5, name=f"dense_dropout_{i}"))

    # Output layer
    model.add(layers.Dense(output_units, name="output"))

    # Compile model
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss='mse',  # Mean Squared Error - tá»‘t cho regression
        metrics=['mae']  # Mean Absolute Error - dá»… hiá»ƒu hÆ¡n
    )

    print(f"âœ… ÄÃ£ build model BiLSTM vá»›i {len(lstm_units)} LSTM layers")

    return model


def print_model_summary(model: models.Sequential) -> None:
    """
    In thÃ´ng tin chi tiáº¿t vá» model

    Giáº£i thÃ­ch: Giá»‘ng nhÆ° "thÃ´ng sá»‘ ká»¹ thuáº­t" - biáº¿t model cÃ³ bao nhiÃªu layers, parameters
    """
    print("\n" + "=" * 60)
    print("ğŸ§  MODEL SUMMARY")
    print("=" * 60)
    model.summary()
    print("=" * 60)

    # Äáº¿m parameters
    total_params = model.count_params()
    trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])
    non_trainable_params = total_params - trainable_params

    print("\nğŸ“Š Thá»‘ng kÃª:")
    print(f"   Total parameters: {total_params:,}")
    print(f"   Trainable: {trainable_params:,}")
    print(f"   Non-trainable: {non_trainable_params:,}")


if __name__ == "__main__":
    # Test function
    input_shape = (60, 1)
    model = build_bilstm_model(input_shape=input_shape)
    print_model_summary(model)
