"""
üîß PREPROCESSING MODULE - X·ª¨ L√ù D·ªÆ LI·ªÜU
-----------------------------------------

Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
- Gi·ªëng nh∆∞ "b·∫øp chu·∫©n b·ªã nguy√™n li·ªáu" - r·ª≠a, c·∫Øt, chia t·ª∑ l·ªá
- D·ªØ li·ªáu th√¥ ‚Üí D·ªØ li·ªáu s·∫°ch ƒë·ªÉ model d·ªÖ h·ªçc

C√°c b∆∞·ªõc:
1. Windowing: T·∫°o sliding windows (nh√¨n l·∫°i l·ªãch s·ª≠)
2. Scaling: Chu·∫©n ho√° v·ªÅ range 0-1
3. Splitting: Chia train/val/test

Tr√°ch nhi·ªám (SoC - Separation of Concerns):
- Ch·ªâ x·ª≠ l√Ω d·ªØ li·ªáu, kh√¥ng l√†m g√¨ kh√°c
"""

from typing import Dict, List, Tuple

import numpy as np
import polars as pl
from sklearn.preprocessing import MinMaxScaler, StandardScaler


# ==================== WINDOWING ====================
def create_windows(
    data: np.ndarray,
    window_size: int = 60,
    predict_steps: int = 1
) -> Tuple[np.ndarray, np.ndarray]:
    """
    T·∫°o sliding windows t·ª´ d·ªØ li·ªáu

    Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
    - Gi·ªëng nh∆∞ "nh√¨n qua c·ª≠a s·ªï l∆∞·ªõt"
    - M·ªói l·∫ßn nh√¨n 60 ng√†y qua ƒë·ªÉ d·ª± ƒëo√°n ng√†y mai
    - C·ª≠a s·ªï tr∆∞·ª£t t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi

    Args:
        data: D·ªØ li·ªáu ƒë·∫ßu v√†o (shape: [n_samples, n_features])
        window_size: S·ªë b∆∞·ªõc nh√¨n l·∫°i (past days)
        predict_steps: S·ªë b∆∞·ªõc d·ª± ƒëo√°n (future days)

    Returns:
        X: D·ªØ li·ªáu ƒë·∫ßu v√†o (shape: [n_windows, window_size, n_features])
        y: D·ªØ li·ªáu m·ª•c ti√™u (shape: [n_windows, predict_steps])

    V√≠ d·ª•:
        data = [10, 20, 30, 40, 50, 60, 70]
        window_size = 3, predict_steps = 1

        K·∫øt qu·∫£:
        X = [[10, 20, 30], [20, 30, 40], [30, 40, 50], [40, 50, 60]]
        y = [[40], [50], [60], [70]]
    """
    X, y = [], []
    n_windows = len(data) - window_size - predict_steps + 1

    for i in range(n_windows):
        X.append(data[i:i + window_size])
        y.append(data[i + window_size:i + window_size + predict_steps])

    X = np.array(X)
    y = np.array(y)

    return X, y


def split_data(
    data: np.ndarray,
    train_ratio: float = 0.7,
    val_ratio: float = 0.15
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Chia d·ªØ li·ªáu th√†nh train/val/test

    Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
    - Gi·ªëng nh∆∞ "chia b√†i" cho c√°c v√°n ch∆°i kh√°c nhau
    - Train: ƒê·∫∑t c√¢u h·ªèi v√† ƒë√°p √°n (h·ªçc)
    - Val: Ki·ªÉm tra sau m·ªói v√°n ƒë·ªÉ ƒëi·ªÅu ch·ªânh
    - Test: Thi cu·ªëi k·ª≥, kh√¥ng ƒë∆∞·ª£c xem ƒë√°p √°n

    Args:
        data: D·ªØ li·ªáu c·∫ßn chia
        train_ratio: T·ª∑ l·ªá train (m·∫∑c ƒë·ªãnh 0.7)
        val_ratio: T·ª∑ l·ªá validation (m·∫∑c ƒë·ªãnh 0.15)
        test_ratio = 1 - train_ratio - val_ratio = 0.15

    Returns:
        train, val, test
    """
    n = len(data)
    train_end = int(n * train_ratio)
    val_end = train_end + int(n * val_ratio)

    train = data[:train_end]
    val = data[train_end:val_end]
    test = data[val_end:]

    print("‚úÖ Chia d·ªØ li·ªáu:")
    print(f"   Train: {len(train)} m·∫´u ({train_ratio:.0%})")
    print(f"   Val:   {len(val)} m·∫´u ({val_ratio:.0%})")
    print(f"   Test:  {len(test)} m·∫´u ({(1-train_ratio-val_ratio):.0%})")

    return train, val, test


# ==================== SCALING ====================
class DataScaler:
    """
    Class ƒë·ªÉ x·ª≠ l√Ω scaling d·ªØ li·ªáu

    Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
    - Gi·ªëng nh∆∞ "ƒë·ªïi ƒë∆°n v·ªã ƒëo" - $50,000 ‚Üí 0.5 (n·∫øu scale 0-1)
    - Model h·ªçc nhanh h∆°n khi s·ªë nh·ªè v√† ƒë·ªìng nh·∫•t
    """

    def __init__(self, scaler_type: str = "minmax"):
        """
        Args:
            scaler_type: "minmax" ho·∫∑c "standard"
        """
        self.scaler_type = scaler_type
        self.scaler = None

        if scaler_type == "minmax":
            self.scaler = MinMaxScaler(feature_range=(0, 1))
        elif scaler_type == "standard":
            self.scaler = StandardScaler()
        else:
            raise ValueError(
                f"Scaler type kh√¥ng h·ª£p l·ªá: {scaler_type}. "
                "Ch·ªçn 'minmax' ho·∫∑c 'standard'."
            )

    def fit_transform(self, data: np.ndarray) -> np.ndarray:
        """
        Fit scaler v√† transform d·ªØ li·ªáu (d√πng cho training)

        Args:
            data: D·ªØ li·ªáu ƒë·∫ßu v√†o (2D array: [n_samples, n_features])

        Returns:
            D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c scale
        """
        if len(data.shape) == 1:
            data = data.reshape(-1, 1)

        scaled_data = self.scaler.fit_transform(data)

        print(f"‚úÖ ƒê√£ fit v√† transform d·ªØ li·ªáu v·ªõi {self.scaler_type} scaler")
        print(f"   Min: {scaled_data.min():.4f}, Max: {scaled_data.max():.4f}")

        return scaled_data

    def transform(self, data: np.ndarray) -> np.ndarray:
        """
        Transform d·ªØ li·ªáu (d√πng cho validation/test)

        L∆∞u √Ω: KH√îNG fit l·∫°i scaler!

        Args:
            data: D·ªØ li·ªáu ƒë·∫ßu v√†o

        Returns:
            D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c scale
        """
        if len(data.shape) == 1:
            data = data.reshape(-1, 1)

        return self.scaler.transform(data)

    def inverse_transform(self, data: np.ndarray) -> np.ndarray:
        """
        Transform ng∆∞·ª£c t·ª´ scaled data v·ªÅ gi√° tr·ªã g·ªëc

        Args:
            data: D·ªØ li·ªáu ƒë√£ scale

        Returns:
            D·ªØ li·ªáu g·ªëc
        """
        if len(data.shape) == 1:
            data = data.reshape(-1, 1)

        return self.scaler.inverse_transform(data).flatten()

    def get_params(self) -> Dict:
        """L·∫•y params c·ªßa scaler"""
        if hasattr(self.scaler, 'data_min_'):
            return {
                "scaler_type": self.scaler_type,
                "data_min_": self.scaler.data_min_.tolist(),
                "data_max_": self.scaler.data_max_.tolist(),
                "data_range_": self.scaler.data_range_.tolist(),
            }
        elif hasattr(self.scaler, 'mean_'):
            return {
                "scaler_type": self.scaler_type,
                "mean_": self.scaler.mean_.tolist(),
                "scale_": self.scaler.scale_.tolist(),
            }
        return {"scaler_type": self.scaler_type}


# ==================== COMPLETE PIPELINE ====================
def prepare_data_for_lstm(
    df: pl.DataFrame,
    features: List[str] = None,
    window_size: int = 60,
    scaler_type: str = "minmax",
    train_ratio: float = 0.7,
    val_ratio: float = 0.15
) -> Dict:
    """
    Pipeline ho√†n ch·ªânh ƒë·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu cho LSTM

    Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
    - Gi·ªëng nh∆∞ "assembly line" - quy tr√¨nh ho√†n ch·ªânh
    - Input: DataFrame th√¥ ‚Üí Output: X_train, X_val, X_test, y_train, y_val, y_test

    Args:
        df: DataFrame v·ªõi c√°c c·ªôt OHLCV
        features: List features s·ª≠ d·ª•ng
        window_size: S·ªë n·∫øn nh√¨n l·∫°i
        scaler_type: Lo·∫°i scaler
        train_ratio: T·ª∑ l·ªá train
        val_ratio: T·ª∑ l·ªá validation

    Returns:
        Dictionary ch·ª©a t·∫•t c·∫£ d·ªØ li·ªáu ƒë√£ chu·∫©n b·ªã
    """
    if features is None:
        features = ["close"]
    
    print("\n" + "=" * 70)
    print("üîß CHU·∫®N B·ªä D·ªÆ LI·ªÜU CHO LSTM")
    print("=" * 70 + "\n")

    # 1. L·∫•y features t·ª´ DataFrame
    feature_data = df.select(features).to_numpy()

    print(f"üìä Shape d·ªØ li·ªáu g·ªëc: {feature_data.shape}")
    print(f"   Features: {features}")

    # 2. Scaling
    scaler = DataScaler(scaler_type=scaler_type)
    scaled_data = scaler.fit_transform(feature_data)

    # 3. Chia train/val/test (TR∆Ø·ªöC khi t·∫°o windows!)
    train_data, val_data, test_data = split_data(scaled_data, train_ratio, val_ratio)

    # 4. T·∫°o windows
    X_train, y_train = create_windows(train_data, window_size)
    X_val, y_val = create_windows(val_data, window_size)
    X_test, y_test = create_windows(test_data, window_size)

    print("\n‚úÖ D·ªØ li·ªáu sau khi t·∫°o windows:")
    print(f"   X_train: {X_train.shape}, y_train: {y_train.shape}")
    print(f"   X_val:   {X_val.shape}, y_val: {y_val.shape}")
    print(f"   X_test:  {X_test.shape}, y_test: {y_test.shape}")
    print("")

    return {
        "X_train": X_train,
        "y_train": y_train,
        "X_val": X_val,
        "y_val": y_val,
        "X_test": X_test,
        "y_test": y_test,
        "scaler": scaler,
    }


if __name__ == "__main__":
    # Test
    import polars as pl
    import numpy as np

    # T·∫°o data gi·∫£
    dates = pl.date_range(
        pl.datetime(2020, 1, 1),
        pl.datetime(2022, 1, 1),
        interval="1d",
        eager=True
    )
    df = pl.DataFrame({
        "datetime": dates,
        "close": np.random.randn(len(dates)).cumsum() + 10000
    })

    result = prepare_data_for_lstm(df, window_size=30)
    print("X_train shape:", result["X_train"].shape)
