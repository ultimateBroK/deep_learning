"""
‚öôÔ∏è CONFIG T·∫¨P TRUNG - CENTRALIZED CONFIGURATION
--------------------------------------------------

Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
- Gi·ªëng nh∆∞ "menu nh√† h√†ng" - t·∫•t c·∫£ l·ª±a ch·ªçn ·ªü 1 n∆°i
- Thay ƒë·ªïi ·ªü ƒë√¢y ‚Üí thay ƒë·ªïi to√†n b·ªô project
- Kh√¥ng c·∫ßn s·ª≠a code ·ªü nhi·ªÅu n∆°i (DRY - Don't Repeat Yourself)

V√≠ d·ª•:
- Mu·ªën thay s·ªë epochs ‚Üí s·ª≠a ·ªü ƒë√¢y, kh√¥ng s·ª≠a trong main.py, notebook, test...
- Mu·ªën thay timeframe ‚Üí s·ª≠a ·ªü ƒë√¢y, t·∫•t c·∫£ t·ª± ƒë·ªông c·∫≠p nh·∫≠t

T·∫¨P TRUNG V√ÄO 15m TIMEFRAME (280K D√íNG DATA)
"""

from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Tuple


# ==================== PROJECT PATHS ====================
# Gi·ªëng nh∆∞ "b·∫£n ƒë·ªì" - bi·∫øt m·ªçi th·ª© n·∫±m ·ªü ƒë√¢u
@dataclass
class Paths:
    """ƒê∆∞·ªùng d·∫´n c√°c th∆∞ m·ª•c trong project"""

    project_root: Path = field(default_factory=lambda: Path(__file__).parent.parent)
    data_dir: Path = field(init=False)
    reports_dir: Path = field(init=False)
    models_dir: Path = field(init=False)
    cache_dir: Path = field(init=False)

    def __post_init__(self):
        self.data_dir = self.project_root / "data"
        self.reports_dir = self.project_root / "reports"
        self.models_dir = self.project_root / "models"
        self.cache_dir = self.data_dir / "cache"  # Cache trong th∆∞ m·ª•c data

        # T·∫°o c√°c th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        for p in [self.data_dir, self.reports_dir, self.models_dir, self.cache_dir]:
            p.mkdir(parents=True, exist_ok=True)


# ==================== DATA CONFIG ====================
# Gi·ªëng nh∆∞ "ƒë·∫∑c t·∫£ nguy√™n li·ªáu" - d√πng lo·∫°i d·ªØ li·ªáu n√†o
@dataclass
class DataConfig:
    """C·∫•u h√¨nh cho d·ªØ li·ªáu"""

    # File d·ªØ li·ªáu
    data_path: str = None  # None = t·ª± ch·ªçn theo timeframe
    timeframe: str = "15m"  # Default l√† 15m ƒë·ªÉ t·∫≠n d·ª•ng data kh·ªßng

    # Gi·ªõi h·∫°n d·ªØ li·ªáu
    limit: int = 50000  # Default l√† 50k d√≤ng cho 15m

    # Features d√πng ƒë·ªÉ d·ª± ƒëo√°n
    features: List[str] = field(default_factory=lambda: ["close"])

    # C√≥ refresh cache kh√¥ng
    refresh_cache: bool = False

    def get_data_file(self) -> Path:
        """L·∫•y ƒë∆∞·ªùng d·∫´n file CSV theo timeframe"""
        if self.data_path:
            return Path(self.data_path)

        tf = self.timeframe.lower()
        paths = Paths()

        if tf == "15m":
            return paths.data_dir / "btc_15m_data_2018_to_2025.csv"
        if tf == "1h":
            return paths.data_dir / "btc_1h_data_2018_to_2025.csv"
        if tf == "4h":
            return paths.data_dir / "btc_4h_data_2018_to_2025.csv"
        return paths.data_dir / "btc_1d_data_2018_to_2025.csv"


# ==================== PREPROCESSING CONFIG ====================
# Gi·ªëng nh∆∞ "c√¥ng th·ª©c ch·∫ø bi·∫øn" - x·ª≠ l√Ω d·ªØ li·ªáu th·∫ø n√†o
@dataclass
class PreprocessingConfig:
    """C·∫•u h√¨nh cho ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu"""

    # Sliding Window
    window_size: int = 240  # Default: 4 ng√†y 15m
    predict_steps: int = 1  # S·ªë b∆∞·ªõc d·ª± ƒëo√°n (th∆∞·ªùng = 1)

    # Scaling
    scaler_type: str = "minmax"  # minmax ho·∫∑c standard

    # Train/Val/Test split
    train_ratio: float = 0.7
    val_ratio: float = 0.15
    # test_ratio = 1 - train_ratio - val_ratio = 0.15


# ==================== MODEL CONFIG ====================
# Gi·ªëng nh∆∞ "thi·∫øt k·∫ø ki·∫øn tr√∫c" - model c√≥ c·∫•u tr√∫c n√†o
@dataclass
class ModelConfig:
    """C·∫•u h√¨nh cho model BiLSTM"""

    # LSTM layers
    lstm_units: List[int] = field(default_factory=lambda: [64, 32])

    # Dropout
    dropout_rate: float = 0.2

    # Dense layers
    dense_units: List[int] = field(default_factory=lambda: [32])

    # Output
    output_units: int = 1

    def get_input_shape(self, window_size: int, n_features: int) -> Tuple[int, int]:
        """L·∫•y shape ƒë·∫ßu v√†o cho model"""
        return (window_size, n_features)


# ==================== TRAINING CONFIG ====================
# Gi·ªëng nh∆∞ "l·ªãch h·ªçc t·∫≠p" - h·ªçc th·∫ø n√†o
@dataclass
class TrainingConfig:
    """C·∫•u h√¨nh cho training"""

    # Training parameters
    epochs: int = 30
    batch_size: int = 32

    # Early stopping
    early_stopping_patience: int = 10

    # Learning rate
    learning_rate: float = 0.001

    # Checkpointing
    save_best_model: bool = True
    checkpoint_dir: str = None  # None = auto

    def get_checkpoint_dir(self, paths: Paths) -> Path:
        """L·∫•y th∆∞ m·ª•c l∆∞u checkpoint"""
        if self.checkpoint_dir:
            return Path(self.checkpoint_dir)
        return paths.models_dir / "checkpoints"


# ==================== RUNTIME CONFIG ====================
# Gi·ªëng nh∆∞ "c·∫•u h√¨nh m√°y t√≠nh" - ch·∫°y th·∫ø n√†o
@dataclass
class RuntimeConfig:
    """C·∫•u h√¨nh cho runtime TensorFlow"""

    # CPU threads (t·ªëi ∆∞u cho CPU AMD)
    intra_op_threads: int = 12  # S·ªë core v·∫≠t l√Ω
    inter_op_threads: int = 2

    # XLA optimization
    enable_xla: bool = True

    # Random seed
    seed: int = 42

    # GPU settings
    use_gpu: bool = False  # False = ch·ªâ d√πng CPU


# ==================== VISUALIZATION CONFIG ====================
# Gi·ªëng nh∆∞ "thi·∫øt k·∫ø slide" - hi·ªÉn th·ªã th·∫ø n√†o
@dataclass
class VisualizationConfig:
    """C·∫•u h√¨nh cho visualization"""

    # Plot style
    style: str = "seaborn-v0_8-darkgrid"

    # DPI (ƒë·ªô ph√¢n gi·∫£i)
    dpi: int = 300

    # Figure size
    default_figsize: Tuple[int, int] = (14, 5)


# ==================== MASTER CONFIG ====================
# Gi·ªëng nh∆∞ "menu ch√≠nh" - t·∫•t c·∫£ config ·ªü 1 n∆°i
@dataclass
class Config:
    """Config t·ªïng h·ª£p cho to√†n b·ªô project"""

    paths: Paths = field(default_factory=Paths)
    data: DataConfig = field(default_factory=DataConfig)
    preprocessing: PreprocessingConfig = field(default_factory=PreprocessingConfig)
    model: ModelConfig = field(default_factory=ModelConfig)
    training: TrainingConfig = field(default_factory=TrainingConfig)
    runtime: RuntimeConfig = field(default_factory=RuntimeConfig)
    visualization: VisualizationConfig = field(default_factory=VisualizationConfig)

    @classmethod
    def from_args(cls, **kwargs) -> "Config":
        """T·∫°o config t·ª´ CLI arguments"""
        config = cls()

        # Data args
        if "data_path" in kwargs:
            config.data.data_path = kwargs["data_path"]
        if "timeframe" in kwargs:
            config.data.timeframe = kwargs["timeframe"]
        if "limit" in kwargs:
            config.data.limit = kwargs["limit"]
        if "refresh_cache" in kwargs:
            config.data.refresh_cache = kwargs["refresh_cache"]
        if "features" in kwargs:
            config.data.features = kwargs["features"]

        # Preprocessing args
        if "window" in kwargs:
            config.preprocessing.window_size = kwargs["window"]

        # Model args
        if "lstm_units" in kwargs:
            config.model.lstm_units = kwargs["lstm_units"]
        if "dropout" in kwargs:
            config.model.dropout_rate = kwargs["dropout"]

        # Training args
        if "epochs" in kwargs:
            config.training.epochs = kwargs["epochs"]
        if "batch_size" in kwargs:
            config.training.batch_size = kwargs["batch_size"]

        # Runtime args
        if "intra_threads" in kwargs:
            config.runtime.intra_op_threads = kwargs["intra_threads"]
        if "seed" in kwargs:
            config.runtime.seed = kwargs["seed"]

        return config

    def summary(self) -> str:
        """In t√≥m t·∫Øt config"""
        lines = [
            "=" * 70,
            "‚öôÔ∏è  CONFIG SUMMARY",
            "=" * 70,
            "",
            "üìÅ DATA:",
            f"  File: {self.data.get_data_file()}",
            f"  Timeframe: {self.data.timeframe}",
            f"  Limit: {self.data.limit} lines",
            f"  Features: {self.data.features}",
            f"  Refresh cache: {self.data.refresh_cache}",
            "",
            "üîß PREPROCESSING:",
            f"  Window size: {self.preprocessing.window_size}",
            f"  Scaler: {self.preprocessing.scaler_type}",
            f"  Train/Val/Test: {self.preprocessing.train_ratio:.0%}/{self.preprocessing.val_ratio:.0%}/{(1-self.preprocessing.train_ratio-self.preprocessing.val_ratio):.0%}",
            "",
            "üß† MODEL:",
            f"  LSTM units: {self.model.lstm_units}",
            f"  Dropout: {self.model.dropout_rate}",
            f"  Dense units: {self.model.dense_units}",
            "",
            "üèãÔ∏è  TRAINING:",
            f"  Epochs: {self.training.epochs}",
            f"  Batch size: {self.training.batch_size}",
            f"  Learning rate: {self.training.learning_rate}",
            "",
            "‚ö° RUNTIME:",
            f"  Intra-op threads: {self.runtime.intra_op_threads}",
            f"  Inter-op threads: {self.runtime.inter_op_threads}",
            f"  XLA: {self.runtime.enable_xla}",
            f"  Seed: {self.runtime.seed}",
            "",
            "=" * 70,
        ]
        return "\n".join(lines)


# ==================== PRESET CONFIGS ====================
# Gi·ªëng nh∆∞ "combo menu" - config c√≥ s·∫µn cho t·ª´ng m·ª•c ƒë√≠ch
# T·∫¨P TRUNG V√ÄO 15m TIMEFRAME (280K D√íNG DATA)

# ==================== SCALPING PRESETS (15m - Ng·∫Øn h·∫°n c·ª±c nhanh) ====================
def get_scalping_ultra_fast_config() -> Config:
    """Scalping si√™u nhanh - cho 15m, d·ª± ƒëo√°n 6 ti·∫øng t·ªõi"""
    config = Config()
    config.data.limit = 10000
    config.preprocessing.window_size = 24  # 6 ti·∫øng 15m
    config.training.epochs = 5
    config.model.lstm_units = [16]
    config.model.dense_units = []
    config.runtime.intra_op_threads = 4
    return config


def get_scalping_fast_config() -> Config:
    """Scalping nhanh - cho 15m, d·ª± ƒëo√°n 12 ti·∫øng t·ªõi"""
    config = Config()
    config.data.limit = 20000
    config.preprocessing.window_size = 48  # 12 ti·∫øng 15m
    config.training.epochs = 10
    config.model.lstm_units = [32, 16]
    config.model.dense_units = [16]
    config.runtime.intra_op_threads = 6
    return config


# ==================== INTRADAY PRESETS (15m - Ng·∫Øn h·∫°n) ====================
def get_intraday_light_config() -> Config:
    """Intraday nh·∫π - cho 15m, d·ª± ƒëo√°n 1 ng√†y t·ªõi"""
    config = Config()
    config.data.limit = 30000
    config.preprocessing.window_size = 96  # 1 ng√†y 15m
    config.training.epochs = 15
    config.training.early_stopping_patience = 5
    config.model.lstm_units = [32, 16]
    config.model.dense_units = [16]
    config.runtime.intra_op_threads = 8
    return config


def get_intraday_balanced_config() -> Config:
    """Intraday c√¢n b·∫±ng - cho 15m, d·ª± ƒëo√°n 1.5 ng√†y t·ªõi"""
    config = Config()
    config.data.limit = 50000
    config.preprocessing.window_size = 144  # 1.5 ng√†y 15m
    config.training.epochs = 25
    config.training.early_stopping_patience = 8
    config.model.lstm_units = [64, 32]
    config.model.dense_units = [32]
    config.runtime.intra_op_threads = 10
    return config


# ==================== SWING PRESETS (15m - Trung h·∫°n) ====================
def get_swing_fast_config() -> Config:
    """Swing nhanh - cho 15m, d·ª± ƒëo√°n 2.5 ng√†y t·ªõi"""
    config = Config()
    config.data.limit = 70000
    config.preprocessing.window_size = 240  # 2.5 ng√†y 15m
    config.training.epochs = 30
    config.training.early_stopping_patience = 10
    config.model.lstm_units = [64, 32]
    config.model.dense_units = [32]
    config.runtime.intra_op_threads = 12
    return config


def get_swing_balanced_config() -> Config:
    """Swing c√¢n b·∫±ng - cho 15m, d·ª± ƒëo√°n 4 ng√†y t·ªõi"""
    config = Config()
    config.data.limit = 100000
    config.preprocessing.window_size = 384  # 4 ng√†y 15m
    config.training.epochs = 50
    config.training.early_stopping_patience = 12
    config.model.lstm_units = [128, 64, 32]
    config.model.dense_units = [64, 32]
    config.runtime.intra_op_threads = 12
    return config


# ==================== LONG-TERM PRESETS (15m - D√†i h·∫°n) ====================
def get_long_term_config() -> Config:
    """Long-term - cho 15m, d·ª± ƒëo√°n 6 ng√†y t·ªõi"""
    config = Config()
    config.data.limit = 150000
    config.preprocessing.window_size = 576  # 6 ng√†y 15m
    config.training.epochs = 80
    config.training.early_stopping_patience = 15
    config.model.lstm_units = [256, 128, 64, 32]
    config.model.dense_units = [128, 64]
    config.runtime.intra_op_threads = 12
    return config


# ==================== PRODUCTION PRESETS (15m - Ch·∫•t l∆∞·ª£ng cao nh·∫•t) ====================
def get_production_config() -> Config:
    """Production - cho 15m, d·ª± ƒëo√°n 8 ng√†y t·ªõi, ch·∫•t l∆∞·ª£ng cao nh·∫•t"""
    config = Config()
    config.data.limit = 200000
    config.preprocessing.window_size = 768  # 8 ng√†y 15m
    config.training.epochs = 100
    config.training.early_stopping_patience = 20
    config.model.lstm_units = [256, 128, 64, 32]
    config.model.dense_units = [128, 64, 32]
    config.runtime.intra_op_threads = 12
    return config


# ==================== LEGACY PRESETS (Cho c√°c timeframe kh√°c) ====================
def get_default_config() -> Config:
    """Config m·∫∑c ƒë·ªãnh - d√†nh cho 15m timeframe (default)"""
    return Config()


def get_fast_config() -> Config:
    """Config nhanh - d√πng cho test/development v·ªõi 15m"""
    config = Config()
    config.data.limit = 20000
    config.preprocessing.window_size = 48
    config.training.epochs = 10
    config.model.lstm_units = [32, 16]
    config.runtime.intra_op_threads = 6
    return config


def get_1h_light_config() -> Config:
    """1h nh·∫π - d√πng cho testing v·ªõi 1h timeframe"""
    config = Config()
    config.data.timeframe = "1h"  # Set timeframe cho 1h
    config.data.limit = 10000
    config.preprocessing.window_size = 48  # 2 ng√†y 1h
    config.training.epochs = 15
    config.model.lstm_units = [32, 16]
    config.model.dense_units = [16]
    config.runtime.intra_op_threads = 8
    return config


def get_4h_balanced_config() -> Config:
    """4h c√¢n b·∫±ng - d√πng cho 4h timeframe"""
    config = Config()
    config.data.timeframe = "4h"  # Set timeframe cho 4h
    config.data.limit = 2000
    config.preprocessing.window_size = 24  # 4 ng√†y 4h
    config.training.epochs = 30
    config.training.early_stopping_patience = 8
    config.model.lstm_units = [64, 32]
    config.model.dense_units = [32]
    config.runtime.intra_op_threads = 10
    return config
