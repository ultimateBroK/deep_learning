"""
‚öôÔ∏è CONFIG T·∫¨P TRUNG - CENTRALIZED CONFIGURATION
--------------------------------------------------

Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi s·ªëng:
- Gi·ªëng nh∆∞ "menu nh√† h√†ng" - t·∫•t c·∫£ l·ª±a ch·ªçn ·ªü 1 n∆°i
- Thay ƒë·ªïi ·ªü ƒë√¢y ‚Üí thay ƒë·ªïi to√†n b·ªô project
- Kh√¥ng c·∫ßn s·ª≠a code ·ªü nhi·ªÅu n∆°i (DRY - Don't Repeat Yourself)

V√≠ d·ª•:
- Mu·ªën thay s·ªë epochs ‚Üí s·ª≠a ·ªü ƒë√¢y, kh√¥ng s·ª≠a trong main.py, notebook, test...
- Mu·ªën thay timeframe ‚Üí s·ª≠a ·ªü ƒë√¢y, t·∫•t c·∫£ t·ª± ƒë·ªông c·∫≠p nh·∫≠t
"""

from dataclasses import dataclass, field
from pathlib import Path
from typing import List


# ==================== PROJECT PATHS ====================
# Gi·ªëng nh∆∞ "b·∫£n ƒë·ªì" - bi·∫øt m·ªçi th·ª© n·∫±m ·ªü ƒë√¢u
@dataclass
class Paths:
    """ƒê∆∞·ªùng d·∫´n c√°c th∆∞ m·ª•c trong project"""

    project_root: Path = field(default_factory=lambda: Path(__file__).parent.parent)
    data_dir: Path = field(init=False)
    reports_dir: Path = field(init=False)
    models_dir: Path = field(init=False)
    cache_dir: Path = field(init=False)

    def __post_init__(self):
        self.data_dir = self.project_root / "data"
        self.reports_dir = self.project_root / "reports"
        self.models_dir = self.project_root / "models"
        self.cache_dir = self.data_dir / "cache"  # Cache trong th∆∞ m·ª•c data

        # T·∫°o c√°c th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        for p in [self.data_dir, self.reports_dir, self.models_dir, self.cache_dir]:
            p.mkdir(parents=True, exist_ok=True)


# ==================== DATA CONFIG ====================
# Gi·ªëng nh∆∞ "ƒë·∫∑c t·∫£ nguy√™n li·ªáu" - d√πng lo·∫°i d·ªØ li·ªáu n√†o
@dataclass
class DataConfig:
    """C·∫•u h√¨nh cho d·ªØ li·ªáu"""

    # File d·ªØ li·ªáu
    data_path: str = None  # None = t·ª± ch·ªçn theo timeframe
    timeframe: str = "1d"  # 1d ho·∫∑c 4h

    # Gi·ªõi h·∫°n d·ªØ li·ªáu
    limit: int = 1500  # L·∫•y N d√≤ng cu·ªëi (<=0 = l·∫•y t·∫•t c·∫£)

    # Features d√πng ƒë·ªÉ d·ª± ƒëo√°n
    features: List[str] = field(default_factory=lambda: ["close"])

    # C√≥ refresh cache kh√¥ng
    refresh_cache: bool = False

    def get_data_file(self) -> Path:
        """L·∫•y ƒë∆∞·ªùng d·∫´n file CSV theo timeframe"""
        if self.data_path:
            return Path(self.data_path)

        tf = self.timeframe.lower()
        paths = Paths()

        if tf == "4h":
            return paths.data_dir / "btc_4h_data_2018_to_2025.csv"
        return paths.data_dir / "btc_1d_data_2018_to_2025.csv"


# ==================== PREPROCESSING CONFIG ====================
# Gi·ªëng nh∆∞ "c√¥ng th·ª©c ch·∫ø bi·∫øn" - x·ª≠ l√Ω d·ªØ li·ªáu th·∫ø n√†o
@dataclass
class PreprocessingConfig:
    """C·∫•u h√¨nh cho ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu"""

    # Sliding Window
    window_size: int = 60  # S·ªë n·∫øn nh√¨n l·∫°i
    predict_steps: int = 1  # S·ªë b∆∞·ªõc d·ª± ƒëo√°n (th∆∞·ªùng = 1)

    # Scaling
    scaler_type: str = "minmax"  # minmax ho·∫∑c standard

    # Train/Val/Test split
    train_ratio: float = 0.7
    val_ratio: float = 0.15
    # test_ratio = 1 - train_ratio - val_ratio = 0.15


# ==================== MODEL CONFIG ====================
# Gi·ªëng nh∆∞ "thi·∫øt k·∫ø ki·∫øn tr√∫c" - model c√≥ c·∫•u tr√∫c n√†o
@dataclass
class ModelConfig:
    """C·∫•u h√¨nh cho model BiLSTM"""

    # LSTM layers
    lstm_units: List[int] = field(default_factory=lambda: [64, 32])

    # Dropout
    dropout_rate: float = 0.2

    # Dense layers
    dense_units: List[int] = field(default_factory=lambda: [16])

    # Output
    output_units: int = 1

    def get_input_shape(self, window_size: int, n_features: int) -> tuple:
        """L·∫•y shape ƒë·∫ßu v√†o cho model"""
        return (window_size, n_features)


# ==================== TRAINING CONFIG ====================
# Gi·ªëng nh∆∞ "l·ªãch h·ªçc t·∫≠p" - h·ªçc th·∫ø n√†o
@dataclass
class TrainingConfig:
    """C·∫•u h√¨nh cho training"""

    # Training parameters
    epochs: int = 20
    batch_size: int = 32

    # Early stopping
    early_stopping_patience: int = 5

    # Learning rate
    learning_rate: float = 0.001

    # Checkpointing
    save_best_model: bool = True
    checkpoint_dir: str = None  # None = auto

    def get_checkpoint_dir(self, paths: Paths) -> Path:
        """L·∫•y th∆∞ m·ª•c l∆∞u checkpoint"""
        if self.checkpoint_dir:
            return Path(self.checkpoint_dir)
        return paths.models_dir / "checkpoints"


# ==================== RUNTIME CONFIG ====================
# Gi·ªëng nh∆∞ "c·∫•u h√¨nh m√°y t√≠nh" - ch·∫°y th·∫ø n√†o
@dataclass
class RuntimeConfig:
    """C·∫•u h√¨nh cho runtime TensorFlow"""

    # CPU threads (t·ªëi ∆∞u cho CPU AMD)
    intra_op_threads: int = 12  # S·ªë core v·∫≠t l√Ω
    inter_op_threads: int = 2

    # XLA optimization
    enable_xla: bool = True

    # Random seed
    seed: int = 42

    # GPU settings
    use_gpu: bool = False  # False = ch·ªâ d√πng CPU


# ==================== VISUALIZATION CONFIG ====================
# Gi·ªëng nh∆∞ "thi·∫øt k·∫ø slide" - hi·ªÉn th·ªã th·∫ø n√†o
@dataclass
class VisualizationConfig:
    """C·∫•u h√¨nh cho visualization"""

    # Plot style
    style: str = "seaborn-v0_8-darkgrid"

    # DPI (ƒë·ªô ph√¢n gi·∫£i)
    dpi: int = 300

    # Figure size
    default_figsize: tuple = (14, 5)


# ==================== MASTER CONFIG ====================
# Gi·ªëng nh∆∞ "menu ch√≠nh" - t·∫•t c·∫£ config ·ªü 1 n∆°i
@dataclass
class Config:
    """Config t·ªïng h·ª£p cho to√†n b·ªô project"""

    paths: Paths = field(default_factory=Paths)
    data: DataConfig = field(default_factory=DataConfig)
    preprocessing: PreprocessingConfig = field(default_factory=PreprocessingConfig)
    model: ModelConfig = field(default_factory=ModelConfig)
    training: TrainingConfig = field(default_factory=TrainingConfig)
    runtime: RuntimeConfig = field(default_factory=RuntimeConfig)
    visualization: VisualizationConfig = field(default_factory=VisualizationConfig)

    @classmethod
    def from_args(cls, **kwargs) -> "Config":
        """T·∫°o config t·ª´ CLI arguments"""
        config = cls()

        # Data args
        if "data_path" in kwargs:
            config.data.data_path = kwargs["data_path"]
        if "timeframe" in kwargs:
            config.data.timeframe = kwargs["timeframe"]
        if "limit" in kwargs:
            config.data.limit = kwargs["limit"]
        if "refresh_cache" in kwargs:
            config.data.refresh_cache = kwargs["refresh_cache"]
        if "features" in kwargs:
            config.data.features = kwargs["features"]

        # Preprocessing args
        if "window" in kwargs:
            config.preprocessing.window_size = kwargs["window"]

        # Model args
        if "lstm_units" in kwargs:
            config.model.lstm_units = kwargs["lstm_units"]
        if "dropout" in kwargs:
            config.model.dropout_rate = kwargs["dropout"]

        # Training args
        if "epochs" in kwargs:
            config.training.epochs = kwargs["epochs"]
        if "batch_size" in kwargs:
            config.training.batch_size = kwargs["batch_size"]

        # Runtime args
        if "intra_threads" in kwargs:
            config.runtime.intra_op_threads = kwargs["intra_threads"]
        if "seed" in kwargs:
            config.runtime.seed = kwargs["seed"]

        return config

    def summary(self) -> str:
        """In t√≥m t·∫Øt config"""
        lines = [
            "=" * 70,
            "‚öôÔ∏è  CONFIG SUMMARY",
            "=" * 70,
            "",
            "üìÅ DATA:",
            f"  File: {self.data.get_data_file()}",
            f"  Timeframe: {self.data.timeframe}",
            f"  Limit: {self.data.limit} lines",
            f"  Features: {self.data.features}",
            f"  Refresh cache: {self.data.refresh_cache}",
            "",
            "üîß PREPROCESSING:",
            f"  Window size: {self.preprocessing.window_size}",
            f"  Scaler: {self.preprocessing.scaler_type}",
            f"  Train/Val/Test: {self.preprocessing.train_ratio:.0%}/{self.preprocessing.val_ratio:.0%}/{(1-self.preprocessing.train_ratio-self.preprocessing.val_ratio):.0%}",
            "",
            "üß† MODEL:",
            f"  LSTM units: {self.model.lstm_units}",
            f"  Dropout: {self.model.dropout_rate}",
            f"  Dense units: {self.model.dense_units}",
            "",
            "üèãÔ∏è  TRAINING:",
            f"  Epochs: {self.training.epochs}",
            f"  Batch size: {self.training.batch_size}",
            f"  Learning rate: {self.training.learning_rate}",
            "",
            "‚ö° RUNTIME:",
            f"  Intra-op threads: {self.runtime.intra_op_threads}",
            f"  Inter-op threads: {self.runtime.inter_op_threads}",
            f"  XLA: {self.runtime.enable_xla}",
            f"  Seed: {self.runtime.seed}",
            "",
            "=" * 70,
        ]
        return "\n".join(lines)


# ==================== PRESET CONFIGS ====================
# Gi·ªëng nh∆∞ "combo menu" - config c√≥ s·∫µn cho t·ª´ng m·ª•c ƒë√≠ch
def get_default_config() -> Config:
    """Config m·∫∑c ƒë·ªãnh - c√¢n b·∫±ng gi·ªØa t·ªëc ƒë·ªô v√† ch·∫•t l∆∞·ª£ng"""
    return Config()


def get_fast_config() -> Config:
    """Config nhanh - d√πng cho test/development"""
    config = Config()
    config.data.limit = 500
    config.preprocessing.window_size = 30
    config.training.epochs = 5
    config.model.lstm_units = [32, 16]
    return config


def get_high_quality_config() -> Config:
    """Config ch·∫•t l∆∞·ª£ng cao - d√πng cho production"""
    config = Config()
    config.data.limit = 3000
    config.preprocessing.window_size = 90
    config.training.epochs = 50
    config.training.early_stopping_patience = 10
    config.model.lstm_units = [128, 64, 32]
    config.model.dense_units = [64, 32]
    return config
